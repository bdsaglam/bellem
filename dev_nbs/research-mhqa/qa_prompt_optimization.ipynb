{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Question Answering Pipeline\n",
    "\n",
    "This notebook implements a DSPy pipeline for optimizing question answering prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellem.utils import set_seed\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.1,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='train')\n",
    "val_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='validation')\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraph(paragraph):\n",
    "    text = paragraph['paragraph_text']\n",
    "    title = paragraph['title']\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "def make_example(record):\n",
    "    supporting_paragraphs = [p for p in record['paragraphs'] if p['is_supporting']]\n",
    "    context = \"\\n\\n\".join([format_paragraph(p) for p in supporting_paragraphs])\n",
    "    return dspy.Example(\n",
    "        question=record['question'],\n",
    "        context=context,\n",
    "        answer=record['answer'],\n",
    "        answers=[record['answer'], *record['answer_aliases']],\n",
    "    ).with_inputs('question', 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [make_example(record) for record in train_ds]\n",
    "valset = [make_example(record) for record in val_ds]\n",
    "len(trainset), len(valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dspy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGenerateAnswer\u001b[39;00m(\u001b[43mdspy\u001b[49m\u001b[38;5;241m.\u001b[39mSignature):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Answer the question based on the given context.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     context \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mInputField(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmay contain relevant facts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dspy' is not defined"
     ]
    }
   ],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer the question based on the given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled QA module\n",
    "uncompiled_program = dspy.ChainOfThought(GenerateAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimization metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/pc/.cache/huggingface/modules/evaluate_modules/metrics/bdsaglam--musique/9f409241d4cc6ea7853124e79cf44954a75900a0a2c0b9d20b909c2396f6b071 (last modified on Tue Jul 23 21:54:03 2024) since it couldn't be found locally at bdsaglam--musique, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from bellem.musique.eval import calculate_metrics\n",
    "\n",
    "def compute_scores(results):\n",
    "    df = pd.DataFrame([{**dict(example), \"predicted_answer\": pred.answer} for example, pred, score in results])\n",
    "    return calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match_str\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    return answer_exact_match_str(pred.answer, example.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_program = Evaluate(\n",
    "    metric=evaluate_answer,\n",
    "    devset=valset,\n",
    "    num_threads=32,\n",
    "    display_progress=True,\n",
    "    return_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_program(uncompiled_program)\n",
    "print(\"Uncompiled Question Answering Scores\")\n",
    "compute_scores(uncompiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact_match': 0.47,\n",
    " 'f1': 0.5911166818682684,\n",
    " 'fuzzy_match': 0.5266666666666666}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 153 / 300  (51.0): 100%|██████████| 300/300 [08:12<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 51.0 for seed -3\n",
      "Scores so far: [51.0]\n",
      "Best score so far: 51.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 180 / 300  (60.0): 100%|██████████| 300/300 [07:29<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 60.0 for seed -2\n",
      "Scores so far: [51.0, 60.0]\n",
      "Best score so far: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/300 [00:25<15:47,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 193 / 300  (64.3): 100%|██████████| 300/300 [06:38<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 64.33 for seed -1\n",
      "Scores so far: [51.0, 60.0, 64.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/300 [00:30<14:47,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 179 / 300  (59.7): 100%|██████████| 300/300 [06:41<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/300 [00:08<14:21,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 174 / 300  (58.0): 100%|██████████| 300/300 [06:44<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:05<14:33,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 182 / 300  (60.7): 100%|██████████| 300/300 [08:49<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/300 [00:13<13:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 300  (62.3): 100%|██████████| 300/300 [06:24<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:04<11:31,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 178 / 300  (59.3): 100%|██████████| 300/300 [06:34<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/300 [00:08<14:33,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 188 / 300  (62.7): 100%|██████████| 300/300 [07:48<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:02<12:53,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 175 / 300  (58.3): 100%|██████████| 300/300 [07:06<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:11<13:38,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 184 / 300  (61.3): 100%|██████████| 300/300 [07:03<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/300 [00:14<12:07,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 184 / 300  (61.3): 100%|██████████| 300/300 [06:49<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 10/300 [00:28<13:32,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 184 / 300  (61.3): 100%|██████████| 300/300 [08:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:02<13:46,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 175 / 300  (58.3): 100%|██████████| 300/300 [08:50<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/300 [00:19<16:06,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 185 / 300  (61.7): 100%|██████████| 300/300 [07:09<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33, 61.67]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:10<13:12,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 181 / 300  (60.3): 100%|██████████| 300/300 [08:19<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33, 61.67, 60.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:13<16:38,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 181 / 300  (60.3): 100%|██████████| 300/300 [28:17<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33, 61.67, 60.33, 60.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:16<20:36,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 186 / 300  (62.0): 100%|██████████| 300/300 [12:39<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33, 61.67, 60.33, 60.33, 62.0]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:29<36:06,  7.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 186 / 300  (62.0): 100%|██████████| 300/300 [12:02<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [51.0, 60.0, 64.33, 59.67, 58.0, 60.67, 62.33, 59.33, 62.67, 58.33, 61.33, 61.33, 61.33, 58.33, 61.67, 60.33, 60.33, 62.0, 62.0]\n",
      "Best score so far: 64.33\n",
      "19 candidate programs found.\n",
      "[('self', Predict(StringSignature(context, question -> reasoning, answer\n",
      "    instructions='Answer the question based on the given context.'\n",
      "    context = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Context:', 'desc': '${context}'})\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
      "    answer = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'output', 'prefix': 'Answer:', 'desc': '${answer}'})\n",
      ")))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 178 / 300  (59.3): 100%|██████████| 300/300 [06:45<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BootstrapFewShotWithRandomSearch Compiled Question Answering Scores\n",
      "{'exact_match': 0.5933333333333334, 'f1': 0.7032057090512973, 'fuzzy_match': 0.6566666666666666}\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter_cls = BootstrapFewShotWithRandomSearch\n",
    "teleprompter = teleprompter_cls(\n",
    "    metric=evaluate_answer,\n",
    "    max_labeled_demos=4,\n",
    "    max_bootstrapped_demos=4,\n",
    ")\n",
    "\n",
    "compiled_program = teleprompter.compile(uncompiled_program, trainset=trainset)\n",
    "compiled_program_filename = f\"compiled-qa-cot-{teleprompter_cls.__name__.lower()}.json\"\n",
    "compiled_program.save(compiled_program_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, compiled_results = evaluate_program(compiled_program)\n",
    "print(f\"{teleprompter_cls.__name__} Compiled Question Answering Scores\")\n",
    "print(compute_scores(compiled_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if float(score) < 0.9] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Context: {example.context}\")\n",
    "        print(f\"Groundtruth Answers: {example.answers}\")\n",
    "        print(f\"Predicted Answer: {pred.answer}\")\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for uncompiled program\\n\\n\")\n",
    "present_errors(uncompiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for compiled program:\")\n",
    "present_errors(compiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "example = trainset[i]\n",
    "pred = compiled_program(context=example.context, question=example.question)\n",
    "example.answers, pred.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
