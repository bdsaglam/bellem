[
    {
        "rev": "workspace",
        "name": null,
        "data": {
            "rev": "workspace",
            "timestamp": null,
            "params": {
                "pipelines/research-mhqa-evaluation/params.yaml": {
                    "data": {
                        "dataset": {
                            "path": "bdsaglam/musique",
                            "name": "answerable",
                            "split": "train"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "system_prompt": "no-role.txt",
                            "user_prompt_template": "cq.txt",
                            "few_shot_examples": "empty.json"
                        },
                        "run": 1
                    }
                }
            },
            "metrics": {
                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                    "data": {
                        "exact_match": 0.586568361921958,
                        "f1": 0.6939285759126523,
                        "fuzzy_match": 0.6545290400240746,
                        "2hops": {
                            "exact_match": 0.6145659432387313,
                            "f1": 0.7205072396670257,
                            "fuzzy_match": 0.6814134668892599
                        },
                        "3hops": {
                            "exact_match": 0.49988602689765216,
                            "f1": 0.6124656855355362,
                            "fuzzy_match": 0.5767038978801003
                        },
                        "4hops": {
                            "exact_match": 0.5676595744680851,
                            "f1": 0.6728926865097078,
                            "fuzzy_match": 0.6161702127659574
                        }
                    }
                }
            },
            "deps": {
                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                    "size": 3711,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                    "size": 1940,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": "71f63c58861ae953592e01953b879386.dir",
                    "size": 67015840,
                    "nfiles": 19939
                },
                "pipelines/research-mhqa-evaluation/report.py": {
                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                    "size": 2148,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": "fd78c589b2033633ad846460cea20f19.dir",
                    "size": 3219986,
                    "nfiles": 19939
                }
            },
            "outs": {
                "data/raw": {
                    "hash": "a7e1e479d1011f988c073cb290929c5c.dir",
                    "size": 5997,
                    "nfiles": 14,
                    "use_cache": true,
                    "is_data_source": true
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": "71f63c58861ae953592e01953b879386.dir",
                    "size": 67015840,
                    "nfiles": 19939,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": "fd78c589b2033633ad846460cea20f19.dir",
                    "size": 3219986,
                    "nfiles": 19939,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                    "hash": "1b26584d39e36356aa87bba95c902d21",
                    "size": 65699378,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                }
            },
            "meta": {}
        },
        "error": null,
        "experiments": null
    },
    {
        "rev": "a2309f0eb0a7965bd478679e36b9afdbf0c51e61",
        "name": "mhqa/custom-prompt",
        "data": {
            "rev": "a2309f0eb0a7965bd478679e36b9afdbf0c51e61",
            "timestamp": "2024-10-16T22:55:07",
            "params": {
                "pipelines/research-mhqa-evaluation/params.yaml": {
                    "data": {
                        "dataset": {
                            "path": "bdsaglam/musique",
                            "name": "answerable",
                            "split": "train"
                        },
                        "qa": {
                            "model": "llama-3-70b-tgi",
                            "temperature": 0.1,
                            "system_prompt": "no-role.txt",
                            "user_prompt_template": "cq.txt",
                            "few_shot_examples": "empty.json"
                        },
                        "run": 1
                    }
                }
            },
            "metrics": {
                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                    "data": {
                        "exact_match": 0.586568361921958,
                        "f1": 0.6939285759126523,
                        "fuzzy_match": 0.6545290400240746,
                        "2hops": {
                            "exact_match": 0.6145659432387313,
                            "f1": 0.7205072396670257,
                            "fuzzy_match": 0.6814134668892599
                        },
                        "3hops": {
                            "exact_match": 0.49988602689765216,
                            "f1": 0.6124656855355362,
                            "fuzzy_match": 0.5767038978801003
                        },
                        "4hops": {
                            "exact_match": 0.5676595744680851,
                            "f1": 0.6728926865097078,
                            "fuzzy_match": 0.6161702127659574
                        }
                    }
                }
            },
            "deps": {
                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                    "size": 3711,
                    "nfiles": null
                },
                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                    "size": 1940,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": "71f63c58861ae953592e01953b879386.dir",
                    "size": 67015840,
                    "nfiles": 19939
                },
                "pipelines/research-mhqa-evaluation/report.py": {
                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                    "size": 2148,
                    "nfiles": null
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": "fd78c589b2033633ad846460cea20f19.dir",
                    "size": 3219986,
                    "nfiles": 19939
                }
            },
            "outs": {
                "data/raw": {
                    "hash": "a7e1e479d1011f988c073cb290929c5c.dir",
                    "size": 5997,
                    "nfiles": 14,
                    "use_cache": true,
                    "is_data_source": true
                },
                "data/generated/research-mhqa-evaluation/qa-results": {
                    "hash": "71f63c58861ae953592e01953b879386.dir",
                    "size": 67015840,
                    "nfiles": 19939,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/evals": {
                    "hash": "fd78c589b2033633ad846460cea20f19.dir",
                    "size": 3219986,
                    "nfiles": 19939,
                    "use_cache": true,
                    "is_data_source": false
                },
                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                    "hash": "1b26584d39e36356aa87bba95c902d21",
                    "size": 65699378,
                    "nfiles": null,
                    "use_cache": true,
                    "is_data_source": false
                }
            },
            "meta": {}
        },
        "error": null,
        "experiments": [
            {
                "revs": [
                    {
                        "rev": "93d5af0d90e4ce9dd042f4aa975c81b23edd3aa8",
                        "name": "silky-buhl",
                        "data": {
                            "rev": "93d5af0d90e4ce9dd042f4aa975c81b23edd3aa8",
                            "timestamp": "2024-10-17T11:02:52",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6,
                                        "f1": 0.713843752193298,
                                        "fuzzy_match": 0.6833333333333333,
                                        "2hops": {
                                            "exact_match": 0.71,
                                            "f1": 0.7992321241976416,
                                            "fuzzy_match": 0.8
                                        },
                                        "3hops": {
                                            "exact_match": 0.56,
                                            "f1": 0.6554929940761143,
                                            "fuzzy_match": 0.61
                                        },
                                        "4hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6868061383061383,
                                            "fuzzy_match": 0.64
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b0930e7277964696321558f6c839f91e.dir",
                                    "size": 1379451,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e3b0e5ddc1817bae14a09d2f2c3dd9d8.dir",
                                    "size": 50302,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b0930e7277964696321558f6c839f91e.dir",
                                    "size": 1379451,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e3b0e5ddc1817bae14a09d2f2c3dd9d8.dir",
                                    "size": 50302,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ac51621f7687dc1f27305846d30d11e3",
                                    "size": 1353922,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/00f97fb18633c37b63bef579e0fd0a1a7e521d50/00f97fb18633c37b63bef579e0fd0a1a7e521d50.out",
                        "pid": 1126591,
                        "returncode": 0,
                        "task_id": "00f97fb18633c37b63bef579e0fd0a1a7e521d50"
                    }
                },
                "name": "silky-buhl"
            },
            {
                "revs": [
                    {
                        "rev": "2a1f59bdeac7a58a2d648f4ae9e6ce3ffbeadac7",
                        "name": "dowie-mene",
                        "data": {
                            "rev": "2a1f59bdeac7a58a2d648f4ae9e6ce3ffbeadac7",
                            "timestamp": "2024-10-17T11:02:52",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6,
                                        "f1": 0.7152773536119089,
                                        "fuzzy_match": 0.6766666666666666,
                                        "2hops": {
                                            "exact_match": 0.71,
                                            "f1": 0.800770585736103,
                                            "fuzzy_match": 0.8
                                        },
                                        "3hops": {
                                            "exact_match": 0.57,
                                            "f1": 0.6659536922015182,
                                            "fuzzy_match": 0.61
                                        },
                                        "4hops": {
                                            "exact_match": 0.52,
                                            "f1": 0.6791077828981057,
                                            "fuzzy_match": 0.62
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8fdf15146ce6cd50c09c0c41ffac1aac.dir",
                                    "size": 1378909,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "a08749df16f6ea48bc8fbd38e326cbb3.dir",
                                    "size": 50286,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8fdf15146ce6cd50c09c0c41ffac1aac.dir",
                                    "size": 1378909,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "a08749df16f6ea48bc8fbd38e326cbb3.dir",
                                    "size": 50286,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ff8a38f6eb2158be52550758b8b01ade",
                                    "size": 1353382,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/0af9b76723add290985848edeb17c3f3c8fd4344/0af9b76723add290985848edeb17c3f3c8fd4344.out",
                        "pid": 1126560,
                        "returncode": 0,
                        "task_id": "0af9b76723add290985848edeb17c3f3c8fd4344"
                    }
                },
                "name": "dowie-mene"
            },
            {
                "revs": [
                    {
                        "rev": "c5bbe83d66e398135e2745a03ef42c52da108ec3",
                        "name": "fishy-hems",
                        "data": {
                            "rev": "c5bbe83d66e398135e2745a03ef42c52da108ec3",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5366666666666666,
                                        "f1": 0.64998556998557,
                                        "fuzzy_match": 0.6066666666666667,
                                        "2hops": {
                                            "exact_match": 0.62,
                                            "f1": 0.7331471861471863,
                                            "fuzzy_match": 0.74
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6404603174603174,
                                            "fuzzy_match": 0.59
                                        },
                                        "4hops": {
                                            "exact_match": 0.45,
                                            "f1": 0.5763492063492063,
                                            "fuzzy_match": 0.49
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a14271ac9fde640841922e4adc8fa914.dir",
                                    "size": 1391801,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f0e1afb73e287a3d3d210e9860666c27.dir",
                                    "size": 49032,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a14271ac9fde640841922e4adc8fa914.dir",
                                    "size": 1391801,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f0e1afb73e287a3d3d210e9860666c27.dir",
                                    "size": 49032,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "4d535b0fc117157b87d87e30d466f2b7",
                                    "size": 1364962,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/5f5a45b5b6bfc67fef3809d97a11b4a8dc6d783e/5f5a45b5b6bfc67fef3809d97a11b4a8dc6d783e.out",
                        "pid": 1126490,
                        "returncode": 0,
                        "task_id": "5f5a45b5b6bfc67fef3809d97a11b4a8dc6d783e"
                    }
                },
                "name": "fishy-hems"
            },
            {
                "revs": [
                    {
                        "rev": "922b7fef42eb2f40eb7ae475c411faa4db6f2094",
                        "name": "fuzzy-suss",
                        "data": {
                            "rev": "922b7fef42eb2f40eb7ae475c411faa4db6f2094",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5333333333333333,
                                        "f1": 0.6567601222307105,
                                        "fuzzy_match": 0.6133333333333333,
                                        "2hops": {
                                            "exact_match": 0.62,
                                            "f1": 0.7445916305916306,
                                            "fuzzy_match": 0.74
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6326771920889568,
                                            "fuzzy_match": 0.58
                                        },
                                        "4hops": {
                                            "exact_match": 0.44,
                                            "f1": 0.593011544011544,
                                            "fuzzy_match": 0.52
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e9b2d578e64e31d55966c171631de782.dir",
                                    "size": 1393069,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "42fced1ad66c1cb1bcd47482433baf75.dir",
                                    "size": 49264,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e9b2d578e64e31d55966c171631de782.dir",
                                    "size": 1393069,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "42fced1ad66c1cb1bcd47482433baf75.dir",
                                    "size": 49264,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "fb5c481fb6f1e85d4fed7d1924c43c96",
                                    "size": 1366442,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/847d1d57b80a1513de8c785e9dbef71190db5a13/847d1d57b80a1513de8c785e9dbef71190db5a13.out",
                        "pid": 1126507,
                        "returncode": 0,
                        "task_id": "847d1d57b80a1513de8c785e9dbef71190db5a13"
                    }
                },
                "name": "fuzzy-suss"
            },
            {
                "revs": [
                    {
                        "rev": "8e5f295f42656ed937d3effbc1f55297dd56c354",
                        "name": "catty-rugs",
                        "data": {
                            "rev": "8e5f295f42656ed937d3effbc1f55297dd56c354",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4166666666666667,
                                        "f1": 0.48646093993152817,
                                        "fuzzy_match": 0.4633333333333333,
                                        "2hops": {
                                            "exact_match": 0.65,
                                            "f1": 0.7087717086834734,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.38,
                                            "f1": 0.4477936507936508,
                                            "fuzzy_match": 0.4
                                        },
                                        "4hops": {
                                            "exact_match": 0.22,
                                            "f1": 0.3028174603174603,
                                            "fuzzy_match": 0.29
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bbe2bf47680dbaf81c5b7afd1bf70bca.dir",
                                    "size": 1429644,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "d60cf198a2fe5b59af766e89efbea746.dir",
                                    "size": 47565,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bbe2bf47680dbaf81c5b7afd1bf70bca.dir",
                                    "size": 1429644,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "d60cf198a2fe5b59af766e89efbea746.dir",
                                    "size": 47565,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "2460e8f49108944a85e53467a459c2ae",
                                    "size": 1401218,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/a24af3a18ea076875a4728744a9562f46069ead5/a24af3a18ea076875a4728744a9562f46069ead5.out",
                        "pid": 1126457,
                        "returncode": 0,
                        "task_id": "a24af3a18ea076875a4728744a9562f46069ead5"
                    }
                },
                "name": "catty-rugs"
            },
            {
                "revs": [
                    {
                        "rev": "8691b2f8d5b488d1c927e6ae8e9f99b443d97a90",
                        "name": "forte-doge",
                        "data": {
                            "rev": "8691b2f8d5b488d1c927e6ae8e9f99b443d97a90",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.42,
                                        "f1": 0.4864991794697678,
                                        "fuzzy_match": 0.47,
                                        "2hops": {
                                            "exact_match": 0.67,
                                            "f1": 0.72093837535014,
                                            "fuzzy_match": 0.71
                                        },
                                        "3hops": {
                                            "exact_match": 0.4,
                                            "f1": 0.4408888888888889,
                                            "fuzzy_match": 0.42
                                        },
                                        "4hops": {
                                            "exact_match": 0.19,
                                            "f1": 0.2976702741702742,
                                            "fuzzy_match": 0.28
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8e74be9c2317fe64b9e102d4e7b0e313.dir",
                                    "size": 1430351,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "57f62f7ab01cbd6b068c8a1f29f4c2df.dir",
                                    "size": 47655,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8e74be9c2317fe64b9e102d4e7b0e313.dir",
                                    "size": 1430351,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "57f62f7ab01cbd6b068c8a1f29f4c2df.dir",
                                    "size": 47655,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "d384a764004322ce68c27345cb1c892b",
                                    "size": 1401919,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/7fee50588f5bae56af8994b649402197a4d5524d/7fee50588f5bae56af8994b649402197a4d5524d.out",
                        "pid": 1126433,
                        "returncode": 0,
                        "task_id": "7fee50588f5bae56af8994b649402197a4d5524d"
                    }
                },
                "name": "forte-doge"
            },
            {
                "revs": [
                    {
                        "rev": "63177f0c0a7a8b356fb1dff28d61b6a0909224fe",
                        "name": "stony-tics",
                        "data": {
                            "rev": "63177f0c0a7a8b356fb1dff28d61b6a0909224fe",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.42333333333333334,
                                        "f1": 0.4897557932263815,
                                        "fuzzy_match": 0.4633333333333333,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.7089939309056956,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.4,
                                            "f1": 0.4491269841269841,
                                            "fuzzy_match": 0.4
                                        },
                                        "4hops": {
                                            "exact_match": 0.21,
                                            "f1": 0.3111464646464646,
                                            "fuzzy_match": 0.29
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "61bf7185b7c0e1a81a86c9f76d15004f.dir",
                                    "size": 1429670,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "72cd4866d92dfbbc9de683ff688b8156.dir",
                                    "size": 47500,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "61bf7185b7c0e1a81a86c9f76d15004f.dir",
                                    "size": 1429670,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "72cd4866d92dfbbc9de683ff688b8156.dir",
                                    "size": 47500,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a16fa8e484d0008d2cd6026bec4b797c",
                                    "size": 1401103,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/cb952caa32cef870c33d257602b74e06be29a840/cb952caa32cef870c33d257602b74e06be29a840.out",
                        "pid": 1126454,
                        "returncode": 0,
                        "task_id": "cb952caa32cef870c33d257602b74e06be29a840"
                    }
                },
                "name": "stony-tics"
            },
            {
                "revs": [
                    {
                        "rev": "4e06c38036dde060e6f1f8afc8fc8b8a5fc44c98",
                        "name": "busty-frit",
                        "data": {
                            "rev": "4e06c38036dde060e6f1f8afc8fc8b8a5fc44c98",
                            "timestamp": "2024-10-17T11:02:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.53,
                                        "f1": 0.6574324194324194,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.64,
                                            "f1": 0.769002886002886,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.5,
                                            "f1": 0.6000966810966811,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.45,
                                            "f1": 0.6031976911976912,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0970265de876c31b0087bba11e428034.dir",
                                    "size": 1392757,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0c448d951b022057c346a1be287d9bbb.dir",
                                    "size": 49330,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0970265de876c31b0087bba11e428034.dir",
                                    "size": 1392757,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0c448d951b022057c346a1be287d9bbb.dir",
                                    "size": 49330,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "9d18d56b43224172f6ca2ca281f1b9ca",
                                    "size": 1366191,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/511fdd646dd90df7c89ee7f775722cf32919f105/511fdd646dd90df7c89ee7f775722cf32919f105.out",
                        "pid": 1126493,
                        "returncode": 0,
                        "task_id": "511fdd646dd90df7c89ee7f775722cf32919f105"
                    }
                },
                "name": "busty-frit"
            },
            {
                "revs": [
                    {
                        "rev": "fadbdf1185ec39f547cbd952e92cd2f5f032a209",
                        "name": "hyoid-iron",
                        "data": {
                            "rev": "fadbdf1185ec39f547cbd952e92cd2f5f032a209",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6928646538646539,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.68,
                                            "f1": 0.7739047619047619,
                                            "fuzzy_match": 0.76
                                        },
                                        "3hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6521746031746031,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.48,
                                            "f1": 0.6525145965145965,
                                            "fuzzy_match": 0.57
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ff5ea1d94d527f7a2e933930d49547cd.dir",
                                    "size": 1327049,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6b80acecfaf44927ac14180c10668909.dir",
                                    "size": 49415,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ff5ea1d94d527f7a2e933930d49547cd.dir",
                                    "size": 1327049,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6b80acecfaf44927ac14180c10668909.dir",
                                    "size": 49415,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "26e5c5da268717dde1b1a267eb20ebed",
                                    "size": 1300337,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/0b8b6b3a262cc752a9b65b2493bcbc7814bea637/0b8b6b3a262cc752a9b65b2493bcbc7814bea637.out",
                        "pid": 1126414,
                        "returncode": 0,
                        "task_id": "0b8b6b3a262cc752a9b65b2493bcbc7814bea637"
                    }
                },
                "name": "hyoid-iron"
            },
            {
                "revs": [
                    {
                        "rev": "e693e1ba8492f4b01b96db204d8d7d5e27f41f9e",
                        "name": "naive-goos",
                        "data": {
                            "rev": "e693e1ba8492f4b01b96db204d8d7d5e27f41f9e",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4633333333333333,
                                        "f1": 0.5411495911495913,
                                        "fuzzy_match": 0.51,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.716984126984127,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.43,
                                            "f1": 0.4971991341991342,
                                            "fuzzy_match": 0.47
                                        },
                                        "4hops": {
                                            "exact_match": 0.3,
                                            "f1": 0.40926551226551217,
                                            "fuzzy_match": 0.36
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7ee607e51ecbc0af31425f437a01a856.dir",
                                    "size": 1420543,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09989b941b0e424b14a419d203770a67.dir",
                                    "size": 47798,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7ee607e51ecbc0af31425f437a01a856.dir",
                                    "size": 1420543,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09989b941b0e424b14a419d203770a67.dir",
                                    "size": 47798,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "b36b2ba0a179ae8fe135c44c81ecb84f",
                                    "size": 1392343,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/41129cf9d1f1f509410862aa494dad707102b73f/41129cf9d1f1f509410862aa494dad707102b73f.out",
                        "pid": 1126424,
                        "returncode": 0,
                        "task_id": "41129cf9d1f1f509410862aa494dad707102b73f"
                    }
                },
                "name": "naive-goos"
            },
            {
                "revs": [
                    {
                        "rev": "e1944978f23ae6c966b73e3353125668b12f0bbb",
                        "name": "yummy-quad",
                        "data": {
                            "rev": "e1944978f23ae6c966b73e3353125668b12f0bbb",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4633333333333333,
                                        "f1": 0.5359581529581529,
                                        "fuzzy_match": 0.5066666666666667,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.7243174603174602,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.42,
                                            "f1": 0.4914285714285714,
                                            "fuzzy_match": 0.45
                                        },
                                        "4hops": {
                                            "exact_match": 0.31,
                                            "f1": 0.3921284271284271,
                                            "fuzzy_match": 0.37
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1262d50de1371862aa6efb70b2e8db9e.dir",
                                    "size": 1420650,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "71982d830f3ea057f9abf952cbdfda0a.dir",
                                    "size": 47894,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1262d50de1371862aa6efb70b2e8db9e.dir",
                                    "size": 1420650,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "71982d830f3ea057f9abf952cbdfda0a.dir",
                                    "size": 47894,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "d8f974c41f68e219c5c8d48146b9754e",
                                    "size": 1392535,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/7abac03ee5c9585d3c6231f1bf67be2c108b79ed/7abac03ee5c9585d3c6231f1bf67be2c108b79ed.out",
                        "pid": 1126418,
                        "returncode": 0,
                        "task_id": "7abac03ee5c9585d3c6231f1bf67be2c108b79ed"
                    }
                },
                "name": "yummy-quad"
            },
            {
                "revs": [
                    {
                        "rev": "a0c7b898c3cec0f2022659e72c505a697df4a002",
                        "name": "gawsy-mays",
                        "data": {
                            "rev": "a0c7b898c3cec0f2022659e72c505a697df4a002",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4666666666666667,
                                        "f1": 0.5407756132756133,
                                        "fuzzy_match": 0.5066666666666667,
                                        "2hops": {
                                            "exact_match": 0.64,
                                            "f1": 0.7126507936507935,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.44,
                                            "f1": 0.5032142857142857,
                                            "fuzzy_match": 0.47
                                        },
                                        "4hops": {
                                            "exact_match": 0.32,
                                            "f1": 0.40646176046176047,
                                            "fuzzy_match": 0.37
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "51cb987403a8720de74e2779bf121272.dir",
                                    "size": 1421628,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aa18065dcf24cdec6f56e64f24635936.dir",
                                    "size": 47883,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "51cb987403a8720de74e2779bf121272.dir",
                                    "size": 1421628,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aa18065dcf24cdec6f56e64f24635936.dir",
                                    "size": 47883,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "c65bea213ab5eae2832324c5b86a38e3",
                                    "size": 1393505,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/0885bfbb39d9936ca65aa99050a901f8cb5e51ab/0885bfbb39d9936ca65aa99050a901f8cb5e51ab.out",
                        "pid": 1126439,
                        "returncode": 0,
                        "task_id": "0885bfbb39d9936ca65aa99050a901f8cb5e51ab"
                    }
                },
                "name": "gawsy-mays"
            },
            {
                "revs": [
                    {
                        "rev": "6d7b5c05c85e0692fcee28d27724680e911fc582",
                        "name": "mired-tier",
                        "data": {
                            "rev": "6d7b5c05c85e0692fcee28d27724680e911fc582",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6896680726680726,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.69,
                                            "f1": 0.7772380952380952,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6492515262515264,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.47,
                                            "f1": 0.6425145965145964,
                                            "fuzzy_match": 0.56
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5a7f5e03dc7d5d013eb0395152119e3f.dir",
                                    "size": 1327389,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "414e8940fdf608553990b39a8f131829.dir",
                                    "size": 49470,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5a7f5e03dc7d5d013eb0395152119e3f.dir",
                                    "size": 1327389,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "414e8940fdf608553990b39a8f131829.dir",
                                    "size": 49470,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "f392b3cc341b634d61ed4b9f9f2a9e48",
                                    "size": 1300744,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d19042d9c7d3206352544ed00dd228cd65d9ce74/d19042d9c7d3206352544ed00dd228cd65d9ce74.out",
                        "pid": 1126407,
                        "returncode": 0,
                        "task_id": "d19042d9c7d3206352544ed00dd228cd65d9ce74"
                    }
                },
                "name": "mired-tier"
            },
            {
                "revs": [
                    {
                        "rev": "1050cd5653b52eb2c936f138bab6cc96791d4b79",
                        "name": "seral-puku",
                        "data": {
                            "rev": "1050cd5653b52eb2c936f138bab6cc96791d4b79",
                            "timestamp": "2024-10-17T11:02:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6894761534761534,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.69,
                                            "f1": 0.7772380952380952,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6569181929181929,
                                            "fuzzy_match": 0.56
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.6342721722721723,
                                            "fuzzy_match": 0.55
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3f00fa53a54c221338de34d1de1eac11.dir",
                                    "size": 1327613,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b47294369cbfcd2e0edeb7fc895f74db.dir",
                                    "size": 49541,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3f00fa53a54c221338de34d1de1eac11.dir",
                                    "size": 1327613,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b47294369cbfcd2e0edeb7fc895f74db.dir",
                                    "size": 49541,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "fd372058fef2786e9ab5584d73869e35",
                                    "size": 1301033,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d89fad7b25b3095a2ee3ecb26b988c1d3080aa79/d89fad7b25b3095a2ee3ecb26b988c1d3080aa79.out",
                        "pid": 1126417,
                        "returncode": 0,
                        "task_id": "d89fad7b25b3095a2ee3ecb26b988c1d3080aa79"
                    }
                },
                "name": "seral-puku"
            },
            {
                "revs": [
                    {
                        "rev": "ff7868f3b666592bc06d8b15682159281f769eb5",
                        "name": "about-hunt",
                        "data": {
                            "rev": "ff7868f3b666592bc06d8b15682159281f769eb5",
                            "timestamp": "2024-10-17T10:59:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6894761534761534,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.69,
                                            "f1": 0.7772380952380952,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6569181929181929,
                                            "fuzzy_match": 0.56
                                        },
                                        "4hops": {
                                            "exact_match": 0.46,
                                            "f1": 0.6342721722721723,
                                            "fuzzy_match": 0.55
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3f00fa53a54c221338de34d1de1eac11.dir",
                                    "size": 1327613,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b47294369cbfcd2e0edeb7fc895f74db.dir",
                                    "size": 49541,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "3f00fa53a54c221338de34d1de1eac11.dir",
                                    "size": 1327613,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "b47294369cbfcd2e0edeb7fc895f74db.dir",
                                    "size": 49541,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "fd372058fef2786e9ab5584d73869e35",
                                    "size": 1301033,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/7edb806a6f207a82a106ec276375150c9ce2d79e/7edb806a6f207a82a106ec276375150c9ce2d79e.out",
                        "pid": 1124743,
                        "returncode": 0,
                        "task_id": "7edb806a6f207a82a106ec276375150c9ce2d79e"
                    }
                },
                "name": "about-hunt"
            },
            {
                "revs": [
                    {
                        "rev": "e35dbea08e6523c1d4aa47352265499506c7d2a5",
                        "name": "uveal-sima",
                        "data": {
                            "rev": "e35dbea08e6523c1d4aa47352265499506c7d2a5",
                            "timestamp": "2024-10-16T23:37:53",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.42,
                                        "f1": 0.4864991794697678,
                                        "fuzzy_match": 0.47,
                                        "2hops": {
                                            "exact_match": 0.67,
                                            "f1": 0.72093837535014,
                                            "fuzzy_match": 0.71
                                        },
                                        "3hops": {
                                            "exact_match": 0.4,
                                            "f1": 0.4408888888888889,
                                            "fuzzy_match": 0.42
                                        },
                                        "4hops": {
                                            "exact_match": 0.19,
                                            "f1": 0.2976702741702742,
                                            "fuzzy_match": 0.28
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8e74be9c2317fe64b9e102d4e7b0e313.dir",
                                    "size": 1430351,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "57f62f7ab01cbd6b068c8a1f29f4c2df.dir",
                                    "size": 47655,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8e74be9c2317fe64b9e102d4e7b0e313.dir",
                                    "size": 1430351,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "57f62f7ab01cbd6b068c8a1f29f4c2df.dir",
                                    "size": 47655,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "d384a764004322ce68c27345cb1c892b",
                                    "size": 1401919,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d6f055e301d460545fc573aab2837bc1b5d452f2/d6f055e301d460545fc573aab2837bc1b5d452f2.out",
                        "pid": 984766,
                        "returncode": 0,
                        "task_id": "d6f055e301d460545fc573aab2837bc1b5d452f2"
                    }
                },
                "name": "uveal-sima"
            },
            {
                "revs": [
                    {
                        "rev": "f481e7bcfe85ac6ee9d979bde7131058c4ccdcdb",
                        "name": "fenny-awls",
                        "data": {
                            "rev": "f481e7bcfe85ac6ee9d979bde7131058c4ccdcdb",
                            "timestamp": "2024-10-16T23:37:51",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.42333333333333334,
                                        "f1": 0.4897557932263815,
                                        "fuzzy_match": 0.4633333333333333,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.7089939309056956,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.4,
                                            "f1": 0.4491269841269841,
                                            "fuzzy_match": 0.4
                                        },
                                        "4hops": {
                                            "exact_match": 0.21,
                                            "f1": 0.3111464646464646,
                                            "fuzzy_match": 0.29
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "61bf7185b7c0e1a81a86c9f76d15004f.dir",
                                    "size": 1429670,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "72cd4866d92dfbbc9de683ff688b8156.dir",
                                    "size": 47500,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "61bf7185b7c0e1a81a86c9f76d15004f.dir",
                                    "size": 1429670,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "72cd4866d92dfbbc9de683ff688b8156.dir",
                                    "size": 47500,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "a16fa8e484d0008d2cd6026bec4b797c",
                                    "size": 1401103,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/23365fe0d2229c42e6e42350187a81cba536d2e4/23365fe0d2229c42e6e42350187a81cba536d2e4.out",
                        "pid": 984770,
                        "returncode": 0,
                        "task_id": "23365fe0d2229c42e6e42350187a81cba536d2e4"
                    }
                },
                "name": "fenny-awls"
            },
            {
                "revs": [
                    {
                        "rev": "857f690dc56d6a9a11c97168037da5e465936325",
                        "name": "famed-polo",
                        "data": {
                            "rev": "857f690dc56d6a9a11c97168037da5e465936325",
                            "timestamp": "2024-10-16T23:37:50",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cot-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4166666666666667,
                                        "f1": 0.48646093993152817,
                                        "fuzzy_match": 0.4633333333333333,
                                        "2hops": {
                                            "exact_match": 0.65,
                                            "f1": 0.7087717086834734,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.38,
                                            "f1": 0.4477936507936508,
                                            "fuzzy_match": 0.4
                                        },
                                        "4hops": {
                                            "exact_match": 0.22,
                                            "f1": 0.3028174603174603,
                                            "fuzzy_match": 0.29
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bbe2bf47680dbaf81c5b7afd1bf70bca.dir",
                                    "size": 1429644,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "d60cf198a2fe5b59af766e89efbea746.dir",
                                    "size": 47565,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "bbe2bf47680dbaf81c5b7afd1bf70bca.dir",
                                    "size": 1429644,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "d60cf198a2fe5b59af766e89efbea746.dir",
                                    "size": 47565,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "2460e8f49108944a85e53467a459c2ae",
                                    "size": 1401218,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/fbaf1fa69d56deecab2624a099e0910c2654b89e/fbaf1fa69d56deecab2624a099e0910c2654b89e.out",
                        "pid": 984777,
                        "returncode": 0,
                        "task_id": "fbaf1fa69d56deecab2624a099e0910c2654b89e"
                    }
                },
                "name": "famed-polo"
            },
            {
                "revs": [
                    {
                        "rev": "ec80e03b5dada66001def73bddf92c9afa7a4d75",
                        "name": "liney-half",
                        "data": {
                            "rev": "ec80e03b5dada66001def73bddf92c9afa7a4d75",
                            "timestamp": "2024-10-16T23:37:14",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4666666666666667,
                                        "f1": 0.5407756132756133,
                                        "fuzzy_match": 0.5066666666666667,
                                        "2hops": {
                                            "exact_match": 0.64,
                                            "f1": 0.7126507936507935,
                                            "fuzzy_match": 0.68
                                        },
                                        "3hops": {
                                            "exact_match": 0.44,
                                            "f1": 0.5032142857142857,
                                            "fuzzy_match": 0.47
                                        },
                                        "4hops": {
                                            "exact_match": 0.32,
                                            "f1": 0.40646176046176047,
                                            "fuzzy_match": 0.37
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "51cb987403a8720de74e2779bf121272.dir",
                                    "size": 1421628,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aa18065dcf24cdec6f56e64f24635936.dir",
                                    "size": 47883,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "51cb987403a8720de74e2779bf121272.dir",
                                    "size": 1421628,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "aa18065dcf24cdec6f56e64f24635936.dir",
                                    "size": 47883,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "c65bea213ab5eae2832324c5b86a38e3",
                                    "size": 1393505,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/f0e4e8ea45c2427537c41ef318b82ada478d6a93/f0e4e8ea45c2427537c41ef318b82ada478d6a93.out",
                        "pid": 984745,
                        "returncode": 0,
                        "task_id": "f0e4e8ea45c2427537c41ef318b82ada478d6a93"
                    }
                },
                "name": "liney-half"
            },
            {
                "revs": [
                    {
                        "rev": "1d98b1c915e6a952177bc6ba07047ef08c5c55f1",
                        "name": "lairy-gimp",
                        "data": {
                            "rev": "1d98b1c915e6a952177bc6ba07047ef08c5c55f1",
                            "timestamp": "2024-10-16T23:37:00",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4633333333333333,
                                        "f1": 0.5411495911495913,
                                        "fuzzy_match": 0.51,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.716984126984127,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.43,
                                            "f1": 0.4971991341991342,
                                            "fuzzy_match": 0.47
                                        },
                                        "4hops": {
                                            "exact_match": 0.3,
                                            "f1": 0.40926551226551217,
                                            "fuzzy_match": 0.36
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7ee607e51ecbc0af31425f437a01a856.dir",
                                    "size": 1420543,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09989b941b0e424b14a419d203770a67.dir",
                                    "size": 47798,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "7ee607e51ecbc0af31425f437a01a856.dir",
                                    "size": 1420543,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09989b941b0e424b14a419d203770a67.dir",
                                    "size": 47798,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "b36b2ba0a179ae8fe135c44c81ecb84f",
                                    "size": 1392343,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/dc09d81c1b6b41d2d8717c885af14c021561c74e/dc09d81c1b6b41d2d8717c885af14c021561c74e.out",
                        "pid": 984760,
                        "returncode": 0,
                        "task_id": "dc09d81c1b6b41d2d8717c885af14c021561c74e"
                    }
                },
                "name": "lairy-gimp"
            },
            {
                "revs": [
                    {
                        "rev": "979b3d57a7dd4a5581d1411fa280c18f0b3e3f9a",
                        "name": "rarer-awls",
                        "data": {
                            "rev": "979b3d57a7dd4a5581d1411fa280c18f0b3e3f9a",
                            "timestamp": "2024-10-16T23:36:59",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cot.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.4633333333333333,
                                        "f1": 0.5359581529581529,
                                        "fuzzy_match": 0.5066666666666667,
                                        "2hops": {
                                            "exact_match": 0.66,
                                            "f1": 0.7243174603174602,
                                            "fuzzy_match": 0.7
                                        },
                                        "3hops": {
                                            "exact_match": 0.42,
                                            "f1": 0.4914285714285714,
                                            "fuzzy_match": 0.45
                                        },
                                        "4hops": {
                                            "exact_match": 0.31,
                                            "f1": 0.3921284271284271,
                                            "fuzzy_match": 0.37
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1262d50de1371862aa6efb70b2e8db9e.dir",
                                    "size": 1420650,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "71982d830f3ea057f9abf952cbdfda0a.dir",
                                    "size": 47894,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "1262d50de1371862aa6efb70b2e8db9e.dir",
                                    "size": 1420650,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "71982d830f3ea057f9abf952cbdfda0a.dir",
                                    "size": 47894,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "d8f974c41f68e219c5c8d48146b9754e",
                                    "size": 1392535,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/baad7987ec9cad9d72752761f7810e65559fa96b/baad7987ec9cad9d72752761f7810e65559fa96b.out",
                        "pid": 984756,
                        "returncode": 0,
                        "task_id": "baad7987ec9cad9d72752761f7810e65559fa96b"
                    }
                },
                "name": "rarer-awls"
            },
            {
                "revs": [
                    {
                        "rev": "7dd16e6d6f50c1d571b8c567250945c145d8c55f",
                        "name": "dowie-airs",
                        "data": {
                            "rev": "7dd16e6d6f50c1d571b8c567250945c145d8c55f",
                            "timestamp": "2024-10-16T23:34:42",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.53,
                                        "f1": 0.6574324194324194,
                                        "fuzzy_match": 0.61,
                                        "2hops": {
                                            "exact_match": 0.64,
                                            "f1": 0.769002886002886,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.5,
                                            "f1": 0.6000966810966811,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.45,
                                            "f1": 0.6031976911976912,
                                            "fuzzy_match": 0.51
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0970265de876c31b0087bba11e428034.dir",
                                    "size": 1392757,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0c448d951b022057c346a1be287d9bbb.dir",
                                    "size": 49330,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "0970265de876c31b0087bba11e428034.dir",
                                    "size": 1392757,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "0c448d951b022057c346a1be287d9bbb.dir",
                                    "size": 49330,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "9d18d56b43224172f6ca2ca281f1b9ca",
                                    "size": 1366191,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/febe1da7426f50f053cd03767ac698fc7bff642f/febe1da7426f50f053cd03767ac698fc7bff642f.out",
                        "pid": 986114,
                        "returncode": 0,
                        "task_id": "febe1da7426f50f053cd03767ac698fc7bff642f"
                    }
                },
                "name": "dowie-airs"
            },
            {
                "revs": [
                    {
                        "rev": "9117f751aa5f5ef880ad083f4b0fce567ea19b76",
                        "name": "surfy-reef",
                        "data": {
                            "rev": "9117f751aa5f5ef880ad083f4b0fce567ea19b76",
                            "timestamp": "2024-10-16T23:34:38",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5333333333333333,
                                        "f1": 0.6567601222307105,
                                        "fuzzy_match": 0.6133333333333333,
                                        "2hops": {
                                            "exact_match": 0.62,
                                            "f1": 0.7445916305916306,
                                            "fuzzy_match": 0.74
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6326771920889568,
                                            "fuzzy_match": 0.58
                                        },
                                        "4hops": {
                                            "exact_match": 0.44,
                                            "f1": 0.593011544011544,
                                            "fuzzy_match": 0.52
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e9b2d578e64e31d55966c171631de782.dir",
                                    "size": 1393069,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "42fced1ad66c1cb1bcd47482433baf75.dir",
                                    "size": 49264,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "e9b2d578e64e31d55966c171631de782.dir",
                                    "size": 1393069,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "42fced1ad66c1cb1bcd47482433baf75.dir",
                                    "size": 49264,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "fb5c481fb6f1e85d4fed7d1924c43c96",
                                    "size": 1366442,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/1664dd48570f8be609cfd07d77db9199c0b1740e/1664dd48570f8be609cfd07d77db9199c0b1740e.out",
                        "pid": 986112,
                        "returncode": 0,
                        "task_id": "1664dd48570f8be609cfd07d77db9199c0b1740e"
                    }
                },
                "name": "surfy-reef"
            },
            {
                "revs": [
                    {
                        "rev": "d3063e01d4434a8aaaeb930e90b744e990d44116",
                        "name": "gulfy-plot",
                        "data": {
                            "rev": "d3063e01d4434a8aaaeb930e90b744e990d44116",
                            "timestamp": "2024-10-16T23:34:17",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5366666666666666,
                                        "f1": 0.64998556998557,
                                        "fuzzy_match": 0.6066666666666667,
                                        "2hops": {
                                            "exact_match": 0.62,
                                            "f1": 0.7331471861471863,
                                            "fuzzy_match": 0.74
                                        },
                                        "3hops": {
                                            "exact_match": 0.54,
                                            "f1": 0.6404603174603174,
                                            "fuzzy_match": 0.59
                                        },
                                        "4hops": {
                                            "exact_match": 0.45,
                                            "f1": 0.5763492063492063,
                                            "fuzzy_match": 0.49
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a14271ac9fde640841922e4adc8fa914.dir",
                                    "size": 1391801,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f0e1afb73e287a3d3d210e9860666c27.dir",
                                    "size": 49032,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "a14271ac9fde640841922e4adc8fa914.dir",
                                    "size": 1391801,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "f0e1afb73e287a3d3d210e9860666c27.dir",
                                    "size": 49032,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "4d535b0fc117157b87d87e30d466f2b7",
                                    "size": 1364962,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/593fb3f1fcce64ee495b483368fdfadac70518da/593fb3f1fcce64ee495b483368fdfadac70518da.out",
                        "pid": 985963,
                        "returncode": 0,
                        "task_id": "593fb3f1fcce64ee495b483368fdfadac70518da"
                    }
                },
                "name": "gulfy-plot"
            },
            {
                "revs": [
                    {
                        "rev": "2f3ea68a8daa392640d8dab5759b73093452a96f",
                        "name": "pseud-kite",
                        "data": {
                            "rev": "2f3ea68a8daa392640d8dab5759b73093452a96f",
                            "timestamp": "2024-10-16T23:31:34",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6133333333333333,
                                        "f1": 0.7212479634031358,
                                        "fuzzy_match": 0.6933333333333334,
                                        "2hops": {
                                            "exact_match": 0.71,
                                            "f1": 0.8024933074588247,
                                            "fuzzy_match": 0.8
                                        },
                                        "3hops": {
                                            "exact_match": 0.6,
                                            "f1": 0.6813015873015873,
                                            "fuzzy_match": 0.64
                                        },
                                        "4hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6799489954489956,
                                            "fuzzy_match": 0.64
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "33784f0bceb4dd557e313c61fdbd0587.dir",
                                    "size": 1378307,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09484868268624c71ade0fa8eb734d21.dir",
                                    "size": 49972,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "33784f0bceb4dd557e313c61fdbd0587.dir",
                                    "size": 1378307,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "09484868268624c71ade0fa8eb734d21.dir",
                                    "size": 49972,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "8f0a652f3d57fa52c8d8777a971fd993",
                                    "size": 1352474,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/70048c569501bbb2ab05413d56cb5f287caad5f2/70048c569501bbb2ab05413d56cb5f287caad5f2.out",
                        "pid": 986116,
                        "returncode": 0,
                        "task_id": "70048c569501bbb2ab05413d56cb5f287caad5f2"
                    }
                },
                "name": "pseud-kite"
            },
            {
                "revs": [
                    {
                        "rev": "1112a690445fa65be802168158d226353347a1e4",
                        "name": "choky-lava",
                        "data": {
                            "rev": "1112a690445fa65be802168158d226353347a1e4",
                            "timestamp": "2024-10-16T23:31:33",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 3
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6,
                                        "f1": 0.713843752193298,
                                        "fuzzy_match": 0.6833333333333333,
                                        "2hops": {
                                            "exact_match": 0.71,
                                            "f1": 0.7992321241976416,
                                            "fuzzy_match": 0.8
                                        },
                                        "3hops": {
                                            "exact_match": 0.56,
                                            "f1": 0.6554929940761143,
                                            "fuzzy_match": 0.61
                                        },
                                        "4hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6868061383061383,
                                            "fuzzy_match": 0.64
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b0930e7277964696321558f6c839f91e.dir",
                                    "size": 1379451,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e3b0e5ddc1817bae14a09d2f2c3dd9d8.dir",
                                    "size": 50302,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "b0930e7277964696321558f6c839f91e.dir",
                                    "size": 1379451,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "e3b0e5ddc1817bae14a09d2f2c3dd9d8.dir",
                                    "size": 50302,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ac51621f7687dc1f27305846d30d11e3",
                                    "size": 1353922,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/92285c5bc3ccd23c220ffe292db8974bed27c13c/92285c5bc3ccd23c220ffe292db8974bed27c13c.out",
                        "pid": 986109,
                        "returncode": 0,
                        "task_id": "92285c5bc3ccd23c220ffe292db8974bed27c13c"
                    }
                },
                "name": "choky-lava"
            },
            {
                "revs": [
                    {
                        "rev": "30286cb112e7517ee573335c5fc1e6d704956f19",
                        "name": "axile-dogy",
                        "data": {
                            "rev": "30286cb112e7517ee573335c5fc1e6d704956f19",
                            "timestamp": "2024-10-16T23:31:13",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role-cte.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "cte-2-shot.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.6,
                                        "f1": 0.7152773536119089,
                                        "fuzzy_match": 0.6766666666666666,
                                        "2hops": {
                                            "exact_match": 0.71,
                                            "f1": 0.800770585736103,
                                            "fuzzy_match": 0.8
                                        },
                                        "3hops": {
                                            "exact_match": 0.57,
                                            "f1": 0.6659536922015182,
                                            "fuzzy_match": 0.61
                                        },
                                        "4hops": {
                                            "exact_match": 0.52,
                                            "f1": 0.6791077828981057,
                                            "fuzzy_match": 0.62
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8fdf15146ce6cd50c09c0c41ffac1aac.dir",
                                    "size": 1378909,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "a08749df16f6ea48bc8fbd38e326cbb3.dir",
                                    "size": 50286,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "8fdf15146ce6cd50c09c0c41ffac1aac.dir",
                                    "size": 1378909,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "a08749df16f6ea48bc8fbd38e326cbb3.dir",
                                    "size": 50286,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "ff8a38f6eb2158be52550758b8b01ade",
                                    "size": 1353382,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/6031a6625123f84c4974c6dfa90b6a9e758ddb78/6031a6625123f84c4974c6dfa90b6a9e758ddb78.out",
                        "pid": 986102,
                        "returncode": 0,
                        "task_id": "6031a6625123f84c4974c6dfa90b6a9e758ddb78"
                    }
                },
                "name": "axile-dogy"
            },
            {
                "revs": [
                    {
                        "rev": "d8706dc1b209c6977dddd6da7a83dfe8b4bd6dc7",
                        "name": "hunky-kite",
                        "data": {
                            "rev": "d8706dc1b209c6977dddd6da7a83dfe8b4bd6dc7",
                            "timestamp": "2024-10-16T23:02:56",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 2
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6928646538646539,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.68,
                                            "f1": 0.7739047619047619,
                                            "fuzzy_match": 0.76
                                        },
                                        "3hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6521746031746031,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.48,
                                            "f1": 0.6525145965145965,
                                            "fuzzy_match": 0.57
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ff5ea1d94d527f7a2e933930d49547cd.dir",
                                    "size": 1327049,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6b80acecfaf44927ac14180c10668909.dir",
                                    "size": 49415,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "ff5ea1d94d527f7a2e933930d49547cd.dir",
                                    "size": 1327049,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "6b80acecfaf44927ac14180c10668909.dir",
                                    "size": 49415,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "26e5c5da268717dde1b1a267eb20ebed",
                                    "size": 1300337,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/d75d215f9b82aee0d2ab9b689977161f1c76b1d6/d75d215f9b82aee0d2ab9b689977161f1c76b1d6.out",
                        "pid": 987302,
                        "returncode": 0,
                        "task_id": "d75d215f9b82aee0d2ab9b689977161f1c76b1d6"
                    }
                },
                "name": "hunky-kite"
            },
            {
                "revs": [
                    {
                        "rev": "15a32a88a80d555169612fdd8c710723fdbc6f9c",
                        "name": "teary-sous",
                        "data": {
                            "rev": "15a32a88a80d555169612fdd8c710723fdbc6f9c",
                            "timestamp": "2024-10-16T23:02:56",
                            "params": {
                                "pipelines/research-mhqa-evaluation/params.yaml": {
                                    "data": {
                                        "dataset": {
                                            "path": "bdsaglam/musique-mini",
                                            "name": "answerable",
                                            "split": "train"
                                        },
                                        "qa": {
                                            "model": "llama-3-70b-tgi",
                                            "temperature": 0.1,
                                            "system_prompt": "no-role.txt",
                                            "user_prompt_template": "cq.txt",
                                            "few_shot_examples": "empty.json"
                                        },
                                        "run": 1
                                    }
                                }
                            },
                            "metrics": {
                                "data/generated/research-mhqa-evaluation/reports/scores.json": {
                                    "data": {
                                        "exact_match": 0.5633333333333334,
                                        "f1": 0.6896680726680726,
                                        "fuzzy_match": 0.6266666666666667,
                                        "2hops": {
                                            "exact_match": 0.69,
                                            "f1": 0.7772380952380952,
                                            "fuzzy_match": 0.77
                                        },
                                        "3hops": {
                                            "exact_match": 0.53,
                                            "f1": 0.6492515262515264,
                                            "fuzzy_match": 0.55
                                        },
                                        "4hops": {
                                            "exact_match": 0.47,
                                            "f1": 0.6425145965145964,
                                            "fuzzy_match": 0.56
                                        }
                                    }
                                }
                            },
                            "deps": {
                                "pipelines/research-mhqa-evaluation/answer_questions.py": {
                                    "hash": "7dab1ff2b395a71e86fb452478cba565",
                                    "size": 3711,
                                    "nfiles": null
                                },
                                "pipelines/research-mhqa-evaluation/evaluate_answers.py": {
                                    "hash": "959c3effd5a2abd57858463f9f8e4471",
                                    "size": 1940,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5a7f5e03dc7d5d013eb0395152119e3f.dir",
                                    "size": 1327389,
                                    "nfiles": 301
                                },
                                "pipelines/research-mhqa-evaluation/report.py": {
                                    "hash": "5e6c18bc9c310dc222026920268cbb6b",
                                    "size": 2148,
                                    "nfiles": null
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "414e8940fdf608553990b39a8f131829.dir",
                                    "size": 49470,
                                    "nfiles": 301
                                }
                            },
                            "outs": {
                                "data/raw": {
                                    "hash": "74d8ead99f886594310991a6d2ea8b2d.dir",
                                    "size": 6610,
                                    "nfiles": 16,
                                    "use_cache": true,
                                    "is_data_source": true
                                },
                                "data/generated/research-mhqa-evaluation/qa-results": {
                                    "hash": "5a7f5e03dc7d5d013eb0395152119e3f.dir",
                                    "size": 1327389,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/evals": {
                                    "hash": "414e8940fdf608553990b39a8f131829.dir",
                                    "size": 49470,
                                    "nfiles": 301,
                                    "use_cache": true,
                                    "is_data_source": false
                                },
                                "data/generated/research-mhqa-evaluation/reports/results.jsonl": {
                                    "hash": "f392b3cc341b634d61ed4b9f9f2a9e48",
                                    "size": 1300744,
                                    "nfiles": null,
                                    "use_cache": true,
                                    "is_data_source": false
                                }
                            },
                            "meta": {}
                        },
                        "error": null,
                        "experiments": null
                    }
                ],
                "executor": {
                    "state": "success",
                    "name": "dvc-task",
                    "local": {
                        "root": null,
                        "log": "/home/pc/Documents/baris/bellem/.dvc/tmp/exps/run/a2a8ae70926e4983261eea857c0536c776c3c3b1/a2a8ae70926e4983261eea857c0536c776c3c3b1.out",
                        "pid": 987372,
                        "returncode": 0,
                        "task_id": "a2a8ae70926e4983261eea857c0536c776c3c3b1"
                    }
                },
                "name": "teary-sous"
            }
        ]
    }
]