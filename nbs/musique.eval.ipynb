{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuSiQue evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp musique.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bellem.text.utils import fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fuzzy_match_metric(prediction: str, references: list[str]) -> float:\n",
    "    return max([float(fuzzy_match(prediction, ref)) for ref in references])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text: str) -> str:\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text: str) -> str:\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text: str) -> str:\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> list[str]:\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact_match(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match (EM) score between a gold answer and a prediction.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score between a gold answer and a prediction.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(\n",
    "    metric_fn: Callable[[str, str], float],\n",
    "    prediction: str,\n",
    "    ground_truths: list[str],\n",
    ") -> float:\n",
    "    \"\"\"Calculate the maximum metric score for a prediction over all ground truths.\"\"\"\n",
    "    scores_for_ground_truths = [metric_fn(prediction, ground_truth) for ground_truth in ground_truths]\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def compute_scores(prediction: str, reference: list[str]) -> dict:\n",
    "    exact_match = metric_max_over_ground_truths(compute_exact_match, prediction, reference)\n",
    "    f1 = metric_max_over_ground_truths(compute_f1, prediction, reference)\n",
    "    fuzzy_match = fuzzy_match_metric(prediction, reference)\n",
    "    return {\"exact_match\": exact_match, \"f1\": f1, \"fuzzy_match\": fuzzy_match}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0, 'f1': 0.5, 'fuzzy_match': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = compute_scores(\"Alexandre the Great\", [\"Alexander the Great\", \"Great Alexander\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def compute_scores_dataframe(dataf: pd.DataFrame) -> pd.DataFrame:\n",
    "    scores = dataf.apply(lambda row: compute_scores(row[\"predicted_answer\"], row[\"answers\"]), axis=1, result_type=\"expand\")\n",
    "    return pd.concat([dataf, scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "      <th>fuzzy_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Alexandre, Alexandre Dumas]</td>\n",
       "      <td>Alexandre Dumas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[North London]</td>\n",
       "      <td>London</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Paris, Paris, France]</td>\n",
       "      <td>Marseille</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        answers predicted_answer  exact_match        f1  \\\n",
       "0  [Alexandre, Alexandre Dumas]  Alexandre Dumas          1.0  1.000000   \n",
       "1                [North London]           London          0.0  0.666667   \n",
       "2        [Paris, Paris, France]        Marseille          0.0  0.000000   \n",
       "\n",
       "   fuzzy_match  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          0.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"answers\": [\"Alexandre\", \"Alexandre Dumas\"],\n",
    "        \"predicted_answer\": \"Alexandre Dumas\",\n",
    "    },\n",
    "    {\n",
    "        \"answers\": [\"North London\"],\n",
    "        \"predicted_answer\": \"London\",\n",
    "    },\n",
    "    {\n",
    "        \"answers\": [\"Paris\", \"Paris, France\"],\n",
    "        \"predicted_answer\": \"Marseille\",\n",
    "    },\n",
    "])\n",
    "compute_scores_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def aggregate_scores(dataf: pd.DataFrame) -> dict:\n",
    "    return dataf[[\"exact_match\", \"f1\", \"fuzzy_match\"]].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.3333333333333333,\n",
       " 'f1': 0.5555555555555555,\n",
       " 'fuzzy_match': 0.3333333333333333}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_scores(compute_scores_dataframe(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
