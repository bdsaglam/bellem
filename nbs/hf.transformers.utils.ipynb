{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hf.transformers.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "from bellem.logging import get_logger\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_model_kwargs(model_kwargs: dict) -> dict:\n",
    "    model_kwargs = deepcopy(model_kwargs)\n",
    "    \n",
    "    # Setup quantization config \n",
    "    if (quantization_config := model_kwargs.get(\"quantization_config\")) and isinstance(quantization_config, dict):\n",
    "        from transformers import BitsAndBytesConfig\n",
    "\n",
    "        model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(**quantization_config)\n",
    "    \n",
    "    # Setup torch dtype\n",
    "    if (torch_dtype := model_kwargs.get(\"torch_dtype\")) and (torch_dtype != \"auto\"):\n",
    "        model_kwargs[\"torch_dtype\"] = getattr(torch, torch_dtype)\n",
    "\n",
    "    # Setup device map\n",
    "    if \"device_map\" not in model_kwargs:\n",
    "        model_kwargs[\"device_map\"] = {\"\": torch.cuda.current_device()}\n",
    "\n",
    "    # Setup flash attention 2\n",
    "    if model_kwargs.get(\"attn_implementation\") == \"flash_attention_2\":\n",
    "        if not is_flash_attn_2_available():\n",
    "            log.warning(\"Flash attention 2 is not available. Falling back to default attention.\")\n",
    "            del model_kwargs[\"attn_implementation\"]\n",
    "    return model_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def load_tokenizer_model(\n",
    "    model_name_or_path: str,\n",
    "    *,\n",
    "    auto_model_cls=None,\n",
    "    **model_kwargs,\n",
    "):\n",
    "\n",
    "    model_kwargs = prepare_model_kwargs(model_kwargs)\n",
    "\n",
    "    if auto_model_cls is None:\n",
    "        if \"-peft\" in model_name_or_path:\n",
    "            from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "            auto_model_cls = AutoPeftModelForCausalLM\n",
    "        else:\n",
    "            auto_model_cls = AutoModelForCausalLM\n",
    "\n",
    "    # Load model\n",
    "    model = auto_model_cls.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    # Load tokenizer\n",
    "    if auto_model_cls == AutoModelForCausalLM:\n",
    "        tokenizer_id = model_name_or_path\n",
    "    else:\n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        if auto_model_cls == AutoPeftModelForCausalLM:\n",
    "            tokenizer_id = model.active_peft_config.base_model_name_or_path\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown auto_model_cls: {auto_model_cls}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, trust_remote_code=True)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def merge_adapters_and_publish(\n",
    "    model_id: str,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    merged_model_id: str=None,\n",
    "):\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    \n",
    "    if isinstance(torch_dtype, str) and torch_dtype != \"auto\":\n",
    "        torch_dtype = getattr(torch, torch_dtype) \n",
    "\n",
    "    log.info(f\"Loading model and tokenizer for {model_id}\")\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    log.info(\"Merging adapters to model...\")\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    if merged_model_id is None:\n",
    "        merged_model_id = f\"{model_id}-merged\"\n",
    "    log.info(f\"Pushing merged model to HF hub as {merged_model_id}\")\n",
    "    model.push_to_hub(merged_model_id)\n",
    "    tokenizer.push_to_hub(merged_model_id)\n",
    "    return merged_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
