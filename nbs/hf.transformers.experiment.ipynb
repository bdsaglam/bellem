{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers utils for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hf.transformers.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "from typing import Any, Callable\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from transformers.pipelines.text_generation import Chat\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "\n",
    "from bellem.hf.transformers.generation import generate\n",
    "from bellem.hf.transformers.utils import load_tokenizer_model\n",
    "from bellem.hf.datasets.utils import load_datasets\n",
    "from bellem.lang.dataset import partition_input_output_messages\n",
    "from bellem.logging import get_logger\n",
    "from bellem.utils import generate_time_id\n",
    "from bellem.ds import NestedDict\n",
    "from bellem.torch.dataset.utils import ListDataset\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_config_for_fp(config: NestedDict):\n",
    "    if not torch.cuda.is_available():\n",
    "        return config\n",
    "\n",
    "    # Set float precision\n",
    "    if config.at(\"pretrained_model.torch_dtype\") in {\"float16\", \"bfloat16\"}:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        gpu_supports_bf = major >= 8\n",
    "        if gpu_supports_bf:\n",
    "            log.info(\"GPU supports bfloat16.\")\n",
    "        else:\n",
    "            log.info(\"GPU does not support bfloat16.\")\n",
    "        \n",
    "        if config.at(\"pretrained_model.torch_dtype\") == \"bfloat16\" and gpu_supports_bf:\n",
    "            log.info(\"Using bfloat16.\")\n",
    "            torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"bfloat16\", True, False, \"bfloat16\")\n",
    "        else:\n",
    "            log.info(\"Using float16.\")\n",
    "            torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"float16\", False, True, \"float16\")\n",
    "    else:\n",
    "        log.info(\"Not using half-precision float.\")\n",
    "        torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (None, None, None, None)\n",
    "\n",
    "    if config.at(\"pretrained_model.torch_dtype\"):\n",
    "        config.set(\"pretrained_model.torch_dtype\", torch_dtype)\n",
    "    if config.at(\"pretrained_model.quantization_config.load_in_4bit\"):\n",
    "        config.set(\"pretrained_model.quantization_config.bnb_4bit_compute_dtype\", bnb_4bit_compute_dtype)\n",
    "    if config.at(\"trainer.training_args.bf16\") or config.at(\"trainer.training_args.fp16\"):\n",
    "        config.set(\"trainer.training_args.bf16\", bf16)\n",
    "        config.set(\"trainer.training_args.fp16\", fp16)\n",
    "    return config\n",
    "\n",
    "def preprocess_config(config: NestedDict):\n",
    "    config = deepcopy(config)\n",
    "\n",
    "    if isinstance(config.at(\"dataset.train\"), dict):\n",
    "        config.set(\"dataset.train\", [config.at(\"dataset.train\")])\n",
    "    if isinstance(config.at(\"dataset.validation\"), dict):\n",
    "        config.set(\"dataset.validation\", [config.at(\"dataset.validation\")])\n",
    "    \n",
    "    if config.at(\"distributed_training\"):\n",
    "        from accelerate import PartialState\n",
    "        config.set(\"pretrained_model.device_map\", {\"\": PartialState().process_index})\n",
    "\n",
    "    config = prepare_config_for_fp(config)\n",
    "    \n",
    "    # Generate unique model id for output model\n",
    "    if (out_model_id := config.at(\"hfhub.model_id\")) and config.at(\"metaconfig.preprocessing.unique_hfhub_model_id\"):\n",
    "        if \"-peft\" not in out_model_id and config.at(\"trainer.lora\"):\n",
    "            out_model_id += \"-peft\"\n",
    "        if wandb_run_id := config.at(\"wandb.id\"):\n",
    "            out_model_id += f\"-{wandb_run_id}\"\n",
    "        else:\n",
    "            out_model_id += f\"-{generate_time_id()}\"\n",
    "        config.set(\"hfhub.model_id\", out_model_id)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'dataset': {'train': [{'path': 'bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai',\n",
       "    'split': 'train'}],\n",
       "  'validation': [{'path': 'bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai',\n",
       "    'split': 'dev[:256]'}]},\n",
       " 'pretrained_model': {'model_name_or_path': 'meta-llama/llama-2-7b-chat-hf',\n",
       "  'torch_dtype': 'bfloat16',\n",
       "  'quantization_config': {'load_in_8bit': False,\n",
       "   'load_in_4bit': True,\n",
       "   'bnb_4bit_quant_type': 'nf4'}},\n",
       " 'trainer': {'packing': False,\n",
       "  'lora': {'lora_alpha': 16,\n",
       "   'lora_dropout': 0.1,\n",
       "   'r': 64,\n",
       "   'bias': 'none',\n",
       "   'task_type': 'CAUSAL_LM'},\n",
       "  'response_template': '[/INST]',\n",
       "  'response_template_context': ' ',\n",
       "  'training_args': {'bf16': True,\n",
       "   'fp16': False,\n",
       "   'group_by_length': True,\n",
       "   'per_device_train_batch_size': 4,\n",
       "   'gradient_accumulation_steps': 2,\n",
       "   'gradient_checkpointing': True,\n",
       "   'max_grad_norm': 0.3,\n",
       "   'weight_decay': 0.001,\n",
       "   'learning_rate': 0.0002,\n",
       "   'lr_scheduler_type': 'cosine',\n",
       "   'warmup_ratio': 0.03,\n",
       "   'optim': 'paged_adamw_32bit',\n",
       "   'max_steps': -1,\n",
       "   'num_train_epochs': 1,\n",
       "   'logging_steps': 25,\n",
       "   'save_steps': 0,\n",
       "   'report_to': 'wandb'}},\n",
       " 'evaluation': {'pipeline': {'batch_size': 8}, 'metric': 'bdsaglam/jer'},\n",
       " 'wandb': {'mode': 'online', 'entity': 'bdsaglam', 'project': 'thesis-kgcons'},\n",
       " 'hfhub': {'model_id': 'bdsaglam/llama-2-7b-chat-jerx-peft-2024-05-29T23-54-56'},\n",
       " 'metaconfig': {'preprocessing': {'resolve_paths': False,\n",
       "   'unique_hfhub_model_id': True}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "\n",
    "import json\n",
    "config = json.loads(\"\"\"\n",
    "{\n",
    "  \"seed\": 42,\n",
    "  \"dataset\": {\n",
    "    \"train\": {\n",
    "      \"path\": \"bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai\",\n",
    "      \"split\": \"train\"\n",
    "    },\n",
    "    \"validation\": {\n",
    "      \"path\": \"bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai\",\n",
    "      \"split\": \"dev[:256]\"\n",
    "    }\n",
    "  },\n",
    "  \"pretrained_model\": {\n",
    "    \"model_name_or_path\": \"meta-llama/llama-2-7b-chat-hf\",\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"quantization_config\": {\n",
    "      \"load_in_8bit\": false,\n",
    "      \"load_in_4bit\": true,\n",
    "      \"bnb_4bit_quant_type\": \"nf4\"\n",
    "    }\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"packing\": false,\n",
    "    \"lora\": {\n",
    "      \"lora_alpha\": 16,\n",
    "      \"lora_dropout\": 0.1,\n",
    "      \"r\": 64,\n",
    "      \"bias\": \"none\",\n",
    "      \"task_type\": \"CAUSAL_LM\"\n",
    "    },\n",
    "    \"response_template\": \"[/INST]\",\n",
    "    \"response_template_context\": \" \",\n",
    "    \"training_args\": {\n",
    "      \"bf16\": true,\n",
    "      \"fp16\": false,\n",
    "      \"group_by_length\": true,\n",
    "      \"per_device_train_batch_size\": 4,\n",
    "      \"gradient_accumulation_steps\": 2,\n",
    "      \"gradient_checkpointing\": true,\n",
    "      \"max_grad_norm\": 0.3,\n",
    "      \"weight_decay\": 0.001,\n",
    "      \"learning_rate\": 0.0002,\n",
    "      \"lr_scheduler_type\": \"cosine\",\n",
    "      \"warmup_ratio\": 0.03,\n",
    "      \"optim\": \"paged_adamw_32bit\",\n",
    "      \"max_steps\": -1,\n",
    "      \"num_train_epochs\": 1,\n",
    "      \"logging_steps\": 25,\n",
    "      \"save_steps\": 0,\n",
    "      \"report_to\": \"wandb\"\n",
    "    }\n",
    "  },\n",
    "  \"evaluation\": {\n",
    "    \"pipeline\": {\n",
    "      \"batch_size\": 8\n",
    "    },\n",
    "    \"metric\": \"bdsaglam/jer\"\n",
    "  },\n",
    "  \"wandb\": {\n",
    "    \"mode\": \"online\",\n",
    "    \"entity\": \"bdsaglam\",\n",
    "    \"project\": \"thesis-kgcons\"\n",
    "  },\n",
    "  \"hfhub\": {\n",
    "    \"model_id\": \"bdsaglam/llama-2-7b-chat-jerx\"\n",
    "  },\n",
    "  \"metaconfig\": {\n",
    "    \"preprocessing\": {\n",
    "      \"resolve_paths\": false,\n",
    "      \"unique_hfhub_model_id\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "preprocess_config(NestedDict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_datacollator(tokenizer, response_template: str | None, response_template_context: str | None = None):\n",
    "    if not response_template:\n",
    "        return None\n",
    "\n",
    "    if response_template_context is None:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}'\")\n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "    else:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}' and context '{response_template_context}'\")\n",
    "        response_template_with_context = response_template_context + response_template\n",
    "        response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[len(response_template_context):] \n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "    \n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_model_for_training(tokenizer, model):\n",
    "    model_id = model.name_or_path.lower()\n",
    "\n",
    "    if \"llama-2\" in model_id:\n",
    "        from bellem.hf.transformers.llama2 import prepare_llama2_for_training\n",
    "\n",
    "        log.info(\"Base model is a llama-2 model, preparing it for training.\")\n",
    "        prepare_llama2_for_training(tokenizer, model)\n",
    "    \n",
    "    elif \"llama-3\" in model_id:\n",
    "        from bellem.hf.transformers.llama3 import prepare_llama3_for_training\n",
    "\n",
    "        log.info(\"Base model is a llama-3 model, preparing it for training.\")\n",
    "        prepare_llama3_for_training(tokenizer, model)\n",
    "    \n",
    "    else:\n",
    "        log.warning(f\"Base model '{model_id}' is not a llama-2 or llama-3 model, no special preparation is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def calculate_token_counts(\n",
    "    tokenizer,\n",
    "    dataset: Dataset,\n",
    "    *,\n",
    "    messages_field: str = \"messages\",\n",
    "):\n",
    "    if messages_field not in dataset.column_names:\n",
    "        raise ValueError(\n",
    "            f\"Dataset must have `{messages_field}` columns if `text_field` is not specified.\"\n",
    "        )\n",
    "\n",
    "    text_field = \"text\"\n",
    "    dataset = dataset.map(\n",
    "        lambda example: {\n",
    "            text_field: tokenizer.apply_chat_template(\n",
    "                example[messages_field],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Inspect token counts\n",
    "    tokenized_train_ds = dataset.map(lambda examples: tokenizer(examples[text_field]), batched=True)\n",
    "    token_counts = [len(input_ids) for input_ids in tokenized_train_ds[\"input_ids\"]]\n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fine_tune(config: NestedDict):\n",
    "    from peft import LoraConfig\n",
    "\n",
    "    # Base model\n",
    "    pretrained_model_config = config[\"pretrained_model\"]\n",
    "    model_id = pretrained_model_config.pop(\"model_name_or_path\")\n",
    "    tokenizer, base_model = load_tokenizer_model(model_id, **pretrained_model_config)\n",
    "    log.info(f\"Loaded base model {model_id}\")\n",
    "\n",
    "    # Prepare model for training\n",
    "    prepare_model_for_training(tokenizer, base_model)\n",
    "\n",
    "    # Train dataset\n",
    "    train_ds = load_datasets(config.at(\"dataset.train\")).shuffle(seed=config.at(\"seed\"))\n",
    "    log.info(f\"Loaded training dataset with {len(train_ds)} samples.\")\n",
    "\n",
    "    # Inspect token counts\n",
    "    dataset_text_field = config.at(\"trainer.dataset_text_field\")\n",
    "    token_counts = calculate_token_counts(tokenizer, train_ds)\n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "\n",
    "    # Supervised fine-tuning\n",
    "    if config.at(\"trainer.max_seq_length\") is None:\n",
    "        config.set(\"trainer.max_seq_length\", ceil(max(token_counts) / 8) * 8)\n",
    "    max_seq_length = config.at(\"trainer.max_seq_length\")\n",
    "    log.info(f\"Setting max_seq_length={max_seq_length}\")\n",
    "\n",
    "    peft_config = LoraConfig(**config.at(\"trainer.lora\", {}))\n",
    "\n",
    "    packing = config.at(\"trainer.packing\", False)\n",
    "\n",
    "    data_collator = make_datacollator(\n",
    "        tokenizer,\n",
    "        config.at(\"trainer.response_template\"),\n",
    "        config.at(\"trainer.response_template_context\"),\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        **config.at(\"trainer.training_args\"),\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        tokenizer=tokenizer,\n",
    "        model=base_model,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_ds,\n",
    "        dataset_text_field=dataset_text_field,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "    )\n",
    "    log.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    log.info(\"Saving model...\")\n",
    "    final_model_id = config.at(\"hfhub.model_id\")\n",
    "    trainer.model.push_to_hub(final_model_id)\n",
    "    tokenizer.push_to_hub(final_model_id)\n",
    "    log.info(f\"Uploaded PEFT adapters to HF Hub as {final_model_id}\")\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_model_for_inference(tokenizer, model):\n",
    "    model_id = model.name_or_path.lower()\n",
    "\n",
    "    if \"llama-2\" in model_id:\n",
    "        from bellem.hf.transformers.llama2 import prepare_llama2_for_inference\n",
    "\n",
    "        log.info(\"Base model is a llama-2 model, preparing it for inference.\")\n",
    "        prepare_llama2_for_inference(tokenizer, model)\n",
    "    \n",
    "    elif \"llama-3\" in model_id:\n",
    "        from bellem.hf.transformers.llama3 import prepare_llama3_for_inference\n",
    "\n",
    "        log.info(\"Base model is a llama-3 model, preparing it for inference.\")\n",
    "        prepare_llama3_for_inference(tokenizer, model)\n",
    "    \n",
    "    else:\n",
    "        log.warning(f\"Base model '{model_id}' is not a llama-2 or llama-3 model, no special preparation is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def _load_tokenizer_model(config: NestedDict):\n",
    "    model_id = config.at(\"hfhub.model_id\")\n",
    "    kwargs = deepcopy(config.get(\"pretrained_model\", {}))\n",
    "    kwargs.pop(\"model_name_or_path\", None)\n",
    "    return load_tokenizer_model(model_id, **kwargs)\n",
    "\n",
    "\n",
    "def make_pipeline(config, tokenizer, model):\n",
    "    prepare_model_for_inference(tokenizer, model)\n",
    "    return pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        **config.at(\"inference.pipeline\", {}),\n",
    "    )\n",
    "\n",
    "def predict(\n",
    "    config,\n",
    "    *,\n",
    "    tokenizer=None,\n",
    "    model=None,\n",
    "):\n",
    "    # Load validation dataset\n",
    "    dataset_config = config.at(\"dataset.validation\")\n",
    "    assert dataset_config, \"Validation dataset is not provided!\"\n",
    "    dataset = load_datasets(dataset_config)\n",
    "    assert len(dataset) > 0, \"Validation dataset is empty!\"\n",
    "\n",
    "    # Ensure the dataset has input/output columns\n",
    "    cols = dataset[0].keys()\n",
    "    if \"input\" not in cols:\n",
    "        if \"messages\" not in dataset.column_names:\n",
    "            raise ValueError(\"Dataset must have `messages` column if `input` column are not provided.\")\n",
    "        dataset = dataset.map(partition_input_output_messages).remove_columns(\"messages\")\n",
    "\n",
    "    # Prepare text generation pipeline\n",
    "    if tokenizer is None or model is None:\n",
    "        tokenizer, model = _load_tokenizer_model(config)\n",
    "\n",
    "    # Set up pipeline\n",
    "    generation_params = config.at(\"inference.generation_params\", {})\n",
    "    if \"max_new_tokens\" not in generation_params:\n",
    "        if \"output\" in dataset.column_names:\n",
    "            token_counts = calculate_token_counts(tokenizer, dataset, messages_field = \"output\")\n",
    "            log.info(f\"Output token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "            generation_params[\"max_new_tokens\"] = ceil(max(token_counts) / 8) * 8\n",
    "        else:\n",
    "            log.info(\"max_new_tokens is not set.\")\n",
    "\n",
    "    pipe = make_pipeline(config, tokenizer, model)\n",
    "\n",
    "    # Generate\n",
    "    # Create chats so that transformers library doesn't override our ListDataset\n",
    "    chats = ListDataset([Chat(messages) for messages in dataset['input']])\n",
    "    generations = generate(\n",
    "        pipe,\n",
    "        chats,\n",
    "        **generation_params,\n",
    "    )\n",
    "\n",
    "    # Create dataframe \n",
    "    dataf = dataset.to_pandas()\n",
    "    dataf[\"generation\"] = generations\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def evaluate_(\n",
    "    config,\n",
    "    *,\n",
    "    tokenizer=None,\n",
    "    model=None,\n",
    "    metric_kwargs: dict | None = None,\n",
    "    output_parse_fn: Callable[[str], Any] | None = None,\n",
    "):\n",
    "    import evaluate\n",
    "\n",
    "    output_parse_fn = output_parse_fn or (lambda x: x)\n",
    "\n",
    "    dataf = predict(config, tokenizer=tokenizer, model=model)\n",
    "\n",
    "    # Parse texts\n",
    "    dataf[\"prediction\"] = dataf[\"generation\"].map(output_parse_fn)\n",
    "    dataf[\"reference\"] = dataf[\"output\"].map(lambda x: x[0]['content']).map(output_parse_fn)\n",
    "\n",
    "    # Compute scores\n",
    "    metric = evaluate.load(config.at(\"evaluation.metric\"))\n",
    "    metric_kwargs = metric_kwargs or {}\n",
    "    scores = metric.compute(\n",
    "        predictions=dataf[\"prediction\"].values,\n",
    "        references=dataf[\"reference\"].values,\n",
    "        **metric_kwargs,\n",
    "    )\n",
    "\n",
    "    return scores, dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
