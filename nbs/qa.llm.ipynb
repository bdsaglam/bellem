{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp qa.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import json\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from bellek.logging import get_logger\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an excellent question-answering system that is trusted around the world. You base your answers solely on the context information provided and not on prior knowledge. You must provide correct step-by-step reasoning for your answers. \n",
    "\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\n",
    "Output format:\n",
    "Your output must be a single line in JSON such as:\n",
    "{\"reasoning\": \"Provide step by step reasoning for the answer.\", \"answer\": \"Provide the final answer in 2-4 words.\"}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"The context information below is provided.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question.\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def make_qa_chat(context: str, question: str) -> list[dict]:\n",
    "    return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": USER_PROMPT.format(context=context, question=question),\n",
    "            },\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def parse_llm_generation(output: str):\n",
    "    return json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QuestionAnsweringResult(BaseModel):\n",
    "    \"\"\"Data model for answering the question.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(description=\"Concise reasoning for the answer.\")\n",
    "    answer: str = Field(description=\"The answer to the question in 2-4 words.\")\n",
    "    raw_output: str = Field(description=\"The raw output from the model.\")\n",
    "\n",
    "\n",
    "def make_question_answer_func(\n",
    "    model_name: str = \"gpt-3.5-turbo\",\n",
    "    client: OpenAI = None,\n",
    "    completion_kwargs: dict | None = None,\n",
    "):\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    if completion_kwargs is None:\n",
    "        completion_kwargs = {}\n",
    "\n",
    "    def func(context: str, question: str) -> QuestionAnsweringResult:\n",
    "        messages = make_qa_chat(context, question)\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            **completion_kwargs,\n",
    "        )\n",
    "        text = chat_completion.choices[0].message.content\n",
    "        output = parse_llm_generation(text)\n",
    "        return QuestionAnsweringResult(answer=output[\"answer\"], reasoning=output[\"reasoning\"], raw_output=text)\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringResult(reasoning='Dominica first competed at the Olympic Games in 1996 and has participated in each Games since then.', answer='1996', raw_output='{\"reasoning\": \"Dominica first competed at the Olympic Games in 1996 and has participated in each Games since then.\", \"answer\": \"1996\"}')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = [ \"Dominica | first competed at | Olympic Games in 1996\", \"Dominica | has participated in | each Games since then\", \"Dominica | has not won | any medals at the Olympic Games\" ]\n",
    "\n",
    "context = \"\\n\".join(triplets)\n",
    "question = \"When did Dominica first compete in Olympic games?\"\n",
    "answer = \"1996\"\n",
    "\n",
    "qa = make_question_answer_func()\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
