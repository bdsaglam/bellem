{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-index knowledge graph index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp llama_index.indices.knowledge_graph.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "from llama_index.constants import GRAPH_STORE_KEY\n",
    "from llama_index.core.base_retriever import BaseRetriever\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.graph_stores.types import GraphStore\n",
    "from llama_index.indices.base import BaseIndex\n",
    "from llama_index.prompts import BasePromptTemplate\n",
    "from llama_index.prompts.default_prompts import DEFAULT_KG_TRIPLET_EXTRACT_PROMPT\n",
    "from llama_index.schema import BaseNode, IndexNode, MetadataMode\n",
    "from llama_index.service_context import ServiceContext\n",
    "from llama_index.storage.docstore.types import RefDocInfo\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.utils import get_tqdm_iterable\n",
    "from bellem.llama_index.data_structs.data_structs import KG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class KnowledgeGraphIndex(BaseIndex[KG]):\n",
    "    \"\"\"Knowledge Graph Index.\n",
    "\n",
    "    Build a KG by extracting triplets, and leveraging the KG during query-time.\n",
    "\n",
    "    Args:\n",
    "        kg_triple_extract_template (BasePromptTemplate): The prompt to use for\n",
    "            extracting triplets.\n",
    "        max_triplets_per_chunk (int): The maximum number of triplets to extract.\n",
    "        service_context (Optional[ServiceContext]): The service context to use.\n",
    "        storage_context (Optional[StorageContext]): The storage context to use.\n",
    "        graph_store (Optional[GraphStore]): The graph store to use.\n",
    "        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
    "        include_embeddings (bool): Whether to include embeddings in the index.\n",
    "            Defaults to False.\n",
    "        max_object_length (int): The maximum length of the object in a triplet.\n",
    "            Defaults to 128.\n",
    "        kg_triplet_extract_fn (Optional[Callable]): The function to use for\n",
    "            extracting triplets. Defaults to None.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    index_struct_cls = KG\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes: Optional[Sequence[BaseNode]] = None,\n",
    "        objects: Optional[Sequence[IndexNode]] = None,\n",
    "        index_struct: Optional[KG] = None,\n",
    "        service_context: Optional[ServiceContext] = None,\n",
    "        storage_context: Optional[StorageContext] = None,\n",
    "        kg_triple_extract_template: Optional[BasePromptTemplate] = None,\n",
    "        max_triplets_per_chunk: int = 10,\n",
    "        include_embeddings: bool = False,\n",
    "        show_progress: bool = False,\n",
    "        max_object_length: int = 128,\n",
    "        kg_triplet_extract_fn: Optional[Callable] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        # need to set parameters before building index in base class.\n",
    "        self.include_embeddings = include_embeddings\n",
    "        self.max_triplets_per_chunk = max_triplets_per_chunk\n",
    "        self.kg_triple_extract_template = (\n",
    "            kg_triple_extract_template or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT\n",
    "        )\n",
    "        # NOTE: Partially format keyword extract template here.\n",
    "        self.kg_triple_extract_template = (\n",
    "            self.kg_triple_extract_template.partial_format(\n",
    "                max_knowledge_triplets=self.max_triplets_per_chunk\n",
    "            )\n",
    "        )\n",
    "        self._max_object_length = max_object_length\n",
    "        self._kg_triplet_extract_fn = kg_triplet_extract_fn\n",
    "\n",
    "        super().__init__(\n",
    "            nodes=nodes,\n",
    "            index_struct=index_struct,\n",
    "            service_context=service_context,\n",
    "            storage_context=storage_context,\n",
    "            show_progress=show_progress,\n",
    "            objects=objects,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # TODO: legacy conversion - remove in next release\n",
    "        if (\n",
    "            len(self.index_struct.table) > 0\n",
    "            and isinstance(self.graph_store, SimpleGraphStore)\n",
    "            and len(self.graph_store._data.graph_dict) == 0\n",
    "        ):\n",
    "            logger.warning(\"Upgrading previously saved KG index to new storage format.\")\n",
    "            self.graph_store._data.graph_dict = self.index_struct.rel_map\n",
    "\n",
    "    @property\n",
    "    def graph_store(self) -> GraphStore:\n",
    "        return self._graph_store\n",
    "\n",
    "    def as_retriever(self, **kwargs: Any) -> BaseRetriever:\n",
    "        from bellem.llama_index.indices.knowledge_graph.retrievers import KGTableRetriever\n",
    "        from llama_index.indices.knowledge_graph.retrievers import KGRetrieverMode\n",
    "        if len(self.index_struct.embedding_dict) > 0 and \"retriever_mode\" not in kwargs:\n",
    "            kwargs[\"retriever_mode\"] = KGRetrieverMode.HYBRID\n",
    "\n",
    "        return KGTableRetriever(self, object_map=self._object_map, **kwargs)\n",
    "\n",
    "    def _extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n",
    "        if self._kg_triplet_extract_fn is not None:\n",
    "            return self._kg_triplet_extract_fn(text)\n",
    "        else:\n",
    "            return self._llm_extract_triplets(text)\n",
    "\n",
    "    def _llm_extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"Extract keywords from text.\"\"\"\n",
    "        response = self._service_context.llm.predict(\n",
    "            self.kg_triple_extract_template,\n",
    "            text=text,\n",
    "        )\n",
    "        return self._parse_triplet_response(\n",
    "            response, max_length=self._max_object_length\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_triplet_response(\n",
    "        response: str, max_length: int = 128\n",
    "    ) -> List[Tuple[str, str, str]]:\n",
    "        knowledge_strs = response.strip().split(\"\\n\")\n",
    "        results = []\n",
    "        for text in knowledge_strs:\n",
    "            if \"(\" not in text or \")\" not in text or text.index(\")\") < text.index(\"(\"):\n",
    "                # skip empty lines and non-triplets\n",
    "                continue\n",
    "            triplet_part = text[text.index(\"(\") + 1 : text.index(\")\")]\n",
    "            tokens = triplet_part.split(\",\")\n",
    "            if len(tokens) != 3:\n",
    "                continue\n",
    "\n",
    "            if any(len(s.encode(\"utf-8\")) > max_length for s in tokens):\n",
    "                # We count byte-length instead of len() for UTF-8 chars,\n",
    "                # will skip if any of the tokens are too long.\n",
    "                # This is normally due to a poorly formatted triplet\n",
    "                # extraction, in more serious KG building cases\n",
    "                # we'll need NLP models to better extract triplets.\n",
    "                continue\n",
    "\n",
    "            subj, pred, obj = map(str.strip, tokens)\n",
    "            if not subj or not pred or not obj:\n",
    "                # skip partial triplets\n",
    "                continue\n",
    "\n",
    "            # Strip double quotes and Capitalize triplets for disambiguation\n",
    "            subj, pred, obj = (\n",
    "                entity.strip('\"').capitalize() for entity in [subj, pred, obj]\n",
    "            )\n",
    "\n",
    "            results.append((subj, pred, obj))\n",
    "        return results\n",
    "\n",
    "    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> KG:\n",
    "        \"\"\"Build the index from nodes.\"\"\"\n",
    "        # do simple concatenation\n",
    "        index_struct = self.index_struct_cls()\n",
    "        nodes_with_progress = get_tqdm_iterable(\n",
    "            nodes, self._show_progress, \"Processing nodes\"\n",
    "        )\n",
    "        for n in nodes_with_progress:\n",
    "            triplets = self._extract_triplets(\n",
    "                n.get_content(metadata_mode=MetadataMode.LLM)\n",
    "            )\n",
    "            logger.debug(f\"> Extracted triplets: {triplets}\")\n",
    "            for triplet in triplets:\n",
    "                subj, _, obj = triplet\n",
    "                self.upsert_triplet(triplet)\n",
    "                index_struct.add_node([subj, obj], n)\n",
    "\n",
    "            if self.include_embeddings:\n",
    "                triplet_texts = [str(t) for t in triplets]\n",
    "\n",
    "                embed_model = self._service_context.embed_model\n",
    "                embed_outputs = embed_model.get_text_embedding_batch(\n",
    "                    triplet_texts, show_progress=self._show_progress\n",
    "                )\n",
    "                for rel_text, rel_embed in zip(triplet_texts, embed_outputs):\n",
    "                    index_struct.add_to_embedding_dict(rel_text, rel_embed)\n",
    "\n",
    "        return index_struct\n",
    "\n",
    "    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:\n",
    "        \"\"\"Insert a document.\"\"\"\n",
    "        for n in nodes:\n",
    "            triplets = self._extract_triplets(\n",
    "                n.get_content(metadata_mode=MetadataMode.LLM)\n",
    "            )\n",
    "            logger.debug(f\"Extracted triplets: {triplets}\")\n",
    "            for triplet in triplets:\n",
    "                subj, _, obj = triplet\n",
    "                triplet_str = str(triplet)\n",
    "                self.upsert_triplet(triplet)\n",
    "                self._index_struct.add_node([subj, obj], n)\n",
    "                if (\n",
    "                    self.include_embeddings\n",
    "                    and triplet_str not in self._index_struct.embedding_dict\n",
    "                ):\n",
    "                    rel_embedding = (\n",
    "                        self._service_context.embed_model.get_text_embedding(\n",
    "                            triplet_str\n",
    "                        )\n",
    "                    )\n",
    "                    self._index_struct.add_to_embedding_dict(triplet_str, rel_embedding)\n",
    "\n",
    "    def upsert_triplet(self, triplet: Tuple[str, str, str]) -> None:\n",
    "        \"\"\"Insert triplets.\n",
    "\n",
    "        Used for manual insertion of KG triplets (in the form\n",
    "        of (subject, relationship, object)).\n",
    "\n",
    "        Args:\n",
    "            triplet (str): Knowledge triplet\n",
    "\n",
    "        \"\"\"\n",
    "        self._graph_store.upsert_triplet(*triplet)\n",
    "\n",
    "    def add_node(self, keywords: List[str], node: BaseNode) -> None:\n",
    "        \"\"\"Add node.\n",
    "\n",
    "        Used for manual insertion of nodes (keyed by keywords).\n",
    "\n",
    "        Args:\n",
    "            keywords (List[str]): Keywords to index the node.\n",
    "            node (Node): Node to be indexed.\n",
    "\n",
    "        \"\"\"\n",
    "        self._index_struct.add_node(keywords, node)\n",
    "        self._docstore.add_documents([node], allow_update=True)\n",
    "\n",
    "    def upsert_triplet_and_node(\n",
    "        self, triplet: Tuple[str, str, str], node: BaseNode\n",
    "    ) -> None:\n",
    "        \"\"\"Upsert KG triplet and node.\n",
    "\n",
    "        Calls both upsert_triplet and add_node.\n",
    "        Behavior is idempotent; if Node already exists,\n",
    "        only triplet will be added.\n",
    "\n",
    "        Args:\n",
    "            keywords (List[str]): Keywords to index the node.\n",
    "            node (Node): Node to be indexed.\n",
    "\n",
    "        \"\"\"\n",
    "        subj, _, obj = triplet\n",
    "        self.upsert_triplet(triplet)\n",
    "        self.add_node([subj, obj], node)\n",
    "\n",
    "    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:\n",
    "        \"\"\"Delete a node.\"\"\"\n",
    "        raise NotImplementedError(\"Delete is not supported for KG index yet.\")\n",
    "\n",
    "    @property\n",
    "    def ref_doc_info(self) -> Dict[str, RefDocInfo]:\n",
    "        \"\"\"Retrieve a dict mapping of ingested documents and their nodes+metadata.\"\"\"\n",
    "        node_doc_ids_sets = list(self._index_struct.table.values())\n",
    "        node_doc_ids = list(set().union(*node_doc_ids_sets))\n",
    "        nodes = self.docstore.get_nodes(node_doc_ids)\n",
    "\n",
    "        all_ref_doc_info = {}\n",
    "        for node in nodes:\n",
    "            ref_node = node.source_node\n",
    "            if not ref_node:\n",
    "                continue\n",
    "\n",
    "            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)\n",
    "            if not ref_doc_info:\n",
    "                continue\n",
    "\n",
    "            all_ref_doc_info[ref_node.node_id] = ref_doc_info\n",
    "        return all_ref_doc_info\n",
    "\n",
    "    def get_networkx_graph(self, limit: int = 100) -> Any:\n",
    "        \"\"\"Get networkx representation of the graph structure.\n",
    "\n",
    "        Args:\n",
    "            limit (int): Number of starting nodes to be included in the graph.\n",
    "\n",
    "        NOTE: This function requires networkx to be installed.\n",
    "        NOTE: This is a beta feature.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import networkx as nx\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install networkx to visualize the graph: `pip install networkx`\"\n",
    "            )\n",
    "\n",
    "        g = nx.Graph()\n",
    "        subjs = list(self.index_struct.table.keys())\n",
    "\n",
    "        # add edges\n",
    "        rel_map = self._graph_store.get_rel_map(subjs=subjs, depth=1, limit=limit)\n",
    "\n",
    "        added_nodes = set()\n",
    "        for keyword in rel_map:\n",
    "            for path in rel_map[keyword]:\n",
    "                subj = keyword\n",
    "                for i in range(0, len(path), 2):\n",
    "                    if i + 2 >= len(path):\n",
    "                        break\n",
    "\n",
    "                    if subj not in added_nodes:\n",
    "                        g.add_node(subj)\n",
    "                        added_nodes.add(subj)\n",
    "\n",
    "                    rel = path[i + 1]\n",
    "                    obj = path[i + 2]\n",
    "\n",
    "                    g.add_edge(subj, obj, label=rel, title=rel)\n",
    "                    subj = obj\n",
    "        return g\n",
    "\n",
    "    @property\n",
    "    def query_context(self) -> Dict[str, Any]:\n",
    "        return {GRAPH_STORE_KEY: self._graph_store}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def patch_knowledge_graph_index():\n",
    "    from llama_index.data_structs.struct_type import IndexStructType\n",
    "    from llama_index.indices.registry import INDEX_STRUCT_TYPE_TO_INDEX_CLASS\n",
    "    INDEX_STRUCT_TYPE_TO_INDEX_CLASS[IndexStructType.KG] = KnowledgeGraphIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
