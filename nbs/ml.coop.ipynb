{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COOP\n",
    "> Prompt Learning with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ml.coop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from clip import clip\n",
    "from bellem.ml.clip import ClipClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        \n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eos_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "        return x\n",
    "\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        clip_model, \n",
    "        tokenizer, \n",
    "        class_names, \n",
    "        ctx_init=None, \n",
    "        n_ctx=None, \n",
    "        class_specific_contexts=False, \n",
    "        class_token_position='end', \n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (ctx_init is None and n_ctx is None), \"Either of ctx_init or n_ctx must be specified\"\n",
    "        n_cls = len(class_names)\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            tokens = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(tokens).type(dtype)\n",
    "            # taking only the part of context corresponding to the given context, i.e. excluding special tokens or padding\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            if class_specific_contexts:\n",
    "                print(\"Initializing class-specific contexts\")\n",
    "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)\n",
    "            else:\n",
    "                print(\"Initializing a generic context\")\n",
    "                ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized\n",
    "\n",
    "        class_names = [name.replace(\"_\", \" \") for name in class_names]\n",
    "        name_lens = [len(tokenizer.encode(name)) for name in class_names]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in class_names]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx \n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "        self.class_token_position = class_token_position\n",
    "\n",
    "    def forward(self):\n",
    "        ctx = self.ctx\n",
    "        if ctx.dim() == 2:\n",
    "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "\n",
    "        if self.class_token_position == \"end\":\n",
    "            prompts = torch.cat(\n",
    "                [\n",
    "                    prefix,  # (n_cls, 1, dim)\n",
    "                    ctx,     # (n_cls, n_ctx, dim)\n",
    "                    suffix,  # (n_cls, *, dim)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "        elif self.class_token_position == \"middle\":\n",
    "            half_n_ctx = self.n_ctx // 2\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
    "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,     # (1, 1, dim)\n",
    "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
    "                        class_i,      # (1, name_len, dim)\n",
    "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
    "                        suffix_i,     # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "        elif self.class_token_position == \"front\":\n",
    "            prompts = []\n",
    "            for i in range(self.n_cls):\n",
    "                name_len = self.name_lens[i]\n",
    "                prefix_i = prefix[i : i + 1, :, :]\n",
    "                class_i = suffix[i : i + 1, :name_len, :]\n",
    "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
    "                ctx_i = ctx[i : i + 1, :, :]\n",
    "                prompt = torch.cat(\n",
    "                    [\n",
    "                        prefix_i,  # (1, 1, dim)\n",
    "                        class_i,   # (1, name_len, dim)\n",
    "                        ctx_i,     # (1, n_ctx, dim)\n",
    "                        suffix_i,  # (1, *, dim)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "            prompts = torch.cat(prompts, dim=0)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClipVisualEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.image_encoder = clip_model.visual\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.image_encoder(image.type(self.dtype))\n",
    "\n",
    "class PromptLearningTextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model, tokenizer, class_names, **kwargs):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(clip_model, tokenizer, class_names, **kwargs)\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "\n",
    "    def forward(self):\n",
    "        prompts = self.prompt_learner()\n",
    "        text_features = self.text_encoder(prompts, self.prompt_learner.tokenized_prompts)\n",
    "        return text_features\n",
    "\n",
    "class PromptLearningClip(nn.Module):\n",
    "    def __init__(self, clip_model, tokenizer, class_names, **kwargs):\n",
    "        super().__init__()\n",
    "        self.visual_encoder = ClipVisualEncoder(clip_model)\n",
    "        self.text_encoder = PromptLearningTextEncoder(clip_model, tokenizer, class_names, **kwargs)\n",
    "        self.head = ClipClassificationHead(clip_model)\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.visual_encoder(image)\n",
    "        text_features = self.text_encoder()\n",
    "        logits = self.head(image_features, text_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_prompt_learning_clip(class_names, clip_model_name=\"ViT-B/32\", prec='fp32', **kwargs):\n",
    "    from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "    clip_model = clip.load(clip_model_name, device='cpu')[0]\n",
    "    if prec == \"fp32\" or prec == \"amp\":\n",
    "        # CLIP's default precision is fp16\n",
    "        clip_model.float()\n",
    "    \n",
    "    print(\"Building prompt learning CLIP\")\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    return PromptLearningClip(clip_model, tokenizer, class_names, **kwargs)\n",
    "\n",
    "def prepare_prompt_learning_clip(model):\n",
    "    print(\"Turning off gradients for all except prompt_learner\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"prompt_learner\" not in name:\n",
    "            param.requires_grad_(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
