{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ml.clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from bellem.ml.vision import TorchVisionTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def load_clip_preprocess(clip_model_name):\n",
    "    from clip import clip\n",
    "    return clip.load(clip_model_name, device='cpu')[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_tfms_from_clip_preprocess(clip_preprocess):\n",
    "    item_tfms = TorchVisionTransform(transforms.Compose(clip_preprocess.transforms[:-2]))\n",
    "    batch_tfms = TorchVisionTransform(transforms.Compose(clip_preprocess.transforms[-2:]))\n",
    "    return item_tfms, batch_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClipClassificationHead(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(clip_model.logit_scale.detach().clone(), requires_grad=True) \n",
    "    \n",
    "    def forward(self, image_features, text_features):\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        logits = self.logit_scale.exp() * (image_features @ text_features.t())\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class ClipZeroShotClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, class_descriptions):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.head = ClipClassificationHead(clip_model)\n",
    "        with torch.inference_mode():\n",
    "            ctf = self.compute_text_features(class_descriptions)\n",
    "        self.class_text_features = nn.Parameter(ctf, requires_grad=False)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image_features = self.clip_model.encode_image(image)\n",
    "        return self.head(image_features, self.class_text_features)\n",
    "\n",
    "    def compute_text_features(self, texts):\n",
    "        device = next(self.clip_model.parameters()).device\n",
    "        text_tokens = clip.tokenize(texts)\n",
    "        text_features = self.clip_model.encode_text(text_tokens.to(device)).float()\n",
    "        return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
